\documentclass{article}
\usepackage{amsmath, amssymb, graphicx, setspace, multirow,multicol,units,float,fullpage}

\doublespacing

\newcommand{\mathsym}[1]{{}}
\newcommand{\unicode}[1]{{}}

\newcounter{mathematicapage}

\graphicspath{{C:/Users/a.quinones/Dropbox/LosAndes/investigacion/Learning_bagdes}}


\begin{document}

\title{The social costs of badges of status and learning} \author{Andr\'es Qui\~nones} \date{May 2019}
\maketitle

\section{Simple learning model}
\label{simple_model}


\begin{table}
	\centering
\begin{tabular}{c|c|c|cl}
	Option 1 & Option 2 & Action & Estimate & ID \\ \hline
	\multirow{2}{*}{V} & \multirow{2}{*}{R}  & V & Q(VR,V) & 0 \\ \cline{3-4} & & R & Q(VR,R) & 1\\ \cline{3-4} \hline
	\multirow{2}{*}{V} & \multirow{2}{*}{0}  & V & Q(V0,V) & 2 \\ \cline{3-4} & & 0 & Q(V0,0) & 3\\ \cline{3-4} \hline
	\multirow{2}{*}{R} & \multirow{2}{*}{0}  & R & Q(R0,R) & 4\\ \cline{3-4} & & 0 & Q(R0,0) & 5 \\ \cline{3-4} \hline
	\multirow{2}{*}{V} & \multirow{2}{*}{V}  & \multirow{2}{*}{V} & \multirow{2}{*}{Q(VV,V)} & 6 \\ & & \\  \hline
	\multirow{2}{*}{R} & \multirow{2}{*}{R}  & \multirow{2}{*}{R} & \multirow{2}{*}{Q(RR,R)} & 7 \\ & & \\  \hline
	\multirow{2}{*}{0} & \multirow{2}{*}{0}  & \multirow{2}{*}{0} & \multirow{2}{*}{Q(00,0)} & 8 \\ & & 
\end{tabular}
	\label{table:States}
	\caption{ State-space and potential action that agent can take. V and R stand for visitor and resident client respectively. Option 1 and 2 are given by the environment, and the action is chosen by the agent among the two options(see main text). The agent learns through estimating rewards obtained from each one of this State-action pairs $Q(s,a)$ depicted in the last column.}
\end{table}



The agent makes this estimates by starting with an incorrect value for each state-action pair and progressively update it as it faces the different states. Specifically, the update is given by
\begin{equation}
	Q(s_t,a_t)\leftarrow Q(s_t,a_t) +\alpha [R_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)],
	\label{equation:updateRule}
\end{equation}

where $R_{t+1}$ is the reward obtained by the agent from playing the action $a$ given it has faced state $s$ at time $t$, and $Q(s_{t+1},a_{t+1})$ is the current estimate of the reward that will be obtained by the agent from playing action $a$ given it will face state $s$ at time $t+1$. It's worth noting that the update rule in expression  includes the use of an existing estimate as well as the return from current reward; thus, the update uses a form of \textit{bootstrapping}. As the agent faces the different states in the environment, the accuracy of the estimates increases and the agent learns to take actions that maximize the rewards it obtains. The update procedure just described correspond to the \textit{Sarsa} algorithm, part of the \textit{time difference} methods in reinforcement learning \cite{sutton_reinforcement_1998}.

So far, we have described how the agent makes estimates of values for the states. But, in order to collect the rewards the agent must have a policy to choose actions from the available options. Obviously, such policy should use the values estimated, and tend to choose actions with higher value. However, the policy must also explore alternative actions, instead of choosing the one with the highest value. That is because exploration will lead to better estimates of values, and in a changing environment, to finding actions with higher value. Thus, the policy of a agent must balance the exploration and exploitation of available actions. For this simple model, we will use as action choosing policy the \textit{soft-max} function. In the case of the market experiment, the choosing policy is given by the probability of choosing action \textit{a} over the alternative \textit{b} given the state encountered is \textit{s}, formally 

\begin{align}
	P(A_t=a|S_t=s)&=\frac{e^{\nicefrac{Q(s,a)}{\tau}}}{e^{\nicefrac{Q(s,a)}{\tau}}+e^{\nicefrac{Q(s,b)}{\tau}}} \\
	P(A_t=b|S_t=s)&=1-P(A_t=a|S_t=s),
\end{align}

where $\tau$ is a parameter that determines how important is the difference in value between the two options when the agent makes the decision of which action to take. The higher the value of $\tau$ the less important the difference is; and thus, the more exploratory the agent is. Figure  shows the probability that the agent chooses one of the two options as a function of the difference in estimated value of the two actions, for different values of $\tau$.


In this simple model three parameters define the learning strategy of agents: $alpha$ the speed of learning, $\gamma$ the importance of future rewards in the estimation of value, and $\tau$ the importance of value in action choosing policy. In a first exploration, we will explore how variation in those parameters influences the capacity of agents to find a solution to the market experiment. Furthermore, using the natural history of cleaner wrasse, we could asses how natural selection changes the values in those parameters - assuming they are inherited - and the learning strategy. 

\subsection*{Ecological environment}

As a first approximation to the ecological environment faced by learning agents, I will assume that potential cleaning options are replaced by one of the three options (resident, visitor, absence) with a certain constant probability. In the preliminary results presented here I will assume that resident and visitor are equally likely to fill the vacant position, they are both picked with probability 0.2; thus the position remains empty with the complementary probability 0.6. As described before, the market experiment assumes that visitors and residents leave the cleaning station with different probabilities when they are not immediately served. To capture this, I will set two parameters that define the probabilities that they leave when the cleaner fails to serve them. If they are served, I assume both types leave. 

\subsection*{Results}
\subsubsection{State-action pairs value estimation}

Figure  shows the change in the estimated value of choosing a visitor and a resident when both are a cleaning option. Different panels present learning series with combinations of values for parameters $\gamma$ and $\tau$. The bottom row of panels, that corresponds to learning series with $\gamma = 0$, show that the estimated values for the two options do not differ from each other. This result is expected given that I assumed both of them to provide the same reward. The top two panel rows where $\gamma > 0 $, in contrast, show that the value estimated for the two options differ. So, visitor and resident are distinguishable as having different values  only when the learning agent includes future rewards as part of the estimate of value of current actions ($\gamma>0$). The difference in estimate between the options is, however, more clear as the value of $\tau$ increases (see Fig.  centre and right columns). With low $\tau$ there is more variation in the estimated values between different learning rounds. Low values of $\tau$ result in learning agents whose decision making gives a height weight to estimated values, avoiding actions with low estimated value. Therefore, the initial rewards obtained during training are more prone to bias the estimated values. The variation in the estimated values in the first column of panels in figure  is evidence that stochasticity plays a big role in the learning process. 



The values of state-action pairs translates into probabilities of taking actions when agents face each state. Figure  depicts the frequency of taking the visitor option, when agents face the resident-visitor state, in the second half of the training when most values have reached a stable state. Scenarios where there is wide variation between the estimates of different learning series (fig.  correspond to variation in the frequency of of choosing a visitor (fig. ). The wide boxes in the boxplot, evidences that the variation stems from the learning series reaching one or two outcomes; either the agent chooses always a visitor or always a resident. In contrast, scenarios where the estimation is consistent among series, the probability of choosing a visitor reaches intermediate values. When the agent does not consider the future in the estimates of value ($\gamma = 0$, red boxes in fig. ) this intermediate value is 0.5, which corresponds to an equal estimate for both actions (fig. ). While,  when the agent considers future moves in the estimates, the visitor option has a higher value than a resident, and the probability of choosing a visitor is above 0.5.


Figure  shows the rewards accumulated in the second half of the learning series, corresponding to the dynamics of figure , and the frequency of a visitor choice in fig. , discriminated by the values used in parameters $\gamma$ and $\tau$. Scenarios where the agent learns to choose visitors over residents do not necessarily yield higher accumulated rewards. Furthermore, the scenarios that reached more accurate and precise value estimations also yielded lower rewards, particularly when $\tau=5$ (see fig.  $\gamma=0$, $\tau = 5$; and $\gamma=0.5$, $\tau=5$). Note that under the current implementation, agents do not only learn to choose visitors over residents; but also they learn to choose visitor and residents over absence of client. Thus, the differences in reward can stem from agents that do not always choose any client over the absence of client. 

In order to find out whether the low accumulated rewards of some of the scenarios was due to the 'silly' decision of choosing nothing over a client; I run learning series 'hard-wiring' the correct decision when agent face either of the two types of client over the absence of clients. Figures , and  show the dynamics of estimated values, the probability of choosing a visitor and the accumulated rewards respectively, of this new implementation. Under this new set-up, the probability of choosing a visitor explains the accumulated rewards of the different scenarios. As for the variation found in the different scenarios, we retain the result from the original set-up, that higher values of $\tau$, and lower values of $\gamma$ favour more accurate estimations of value. 



\subsubsection{Action value estimation}
\label{sec:actions}






\subsubsection{Negative reinforcement}




\section{A more realistic model}


\begin{equation}
Q=\sum_{i}^{n} f_{i} w_{i}
\label{equation:action_value_gen}
\end{equation}

where $f_i$ are quantifications of client's morphological features; $w_i$ are the estimated weights of those features and $n$ is the number of features that the agent is using for the estimation. 

Each one of the weights is updated after the agent obtains reward and sees the following state. This update is determined by the difference between the estimated value and the 'observed' value, weighted by the morphological feature of the current client it is associated with. This way, the change is larger when the estimated value is far from the observed value, and weights associated with higher quantities of  the morphological features are blame more for the mismatch. It's worth noting that the 'observed' value is a combination of the reward obtained from the current choice and the estimation of value for the future choice. In that sense, just like in the simple case, the 'observed' value is not strictly observed. It is partly bootstrapped by the estimate of the future value. Formally, the update is given by

\begin{equation}
w_i\leftarrow w_i +\alpha [R_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]f_i,
\end{equation}


\subsubsection{State-action pair value estimation}
Given that for estimating value of state-action action pairs agents must assign a value to the pair of clients, rather than just to an individual client, we allow the function approximator to use as inputs the morphological features of both clients that are presented at the same time. Thus, the agent updates weights for the morphological features of two clients. Furthermore, the agent must estimate value for two potential actions; that is choosing each one of the two clients. To allow these two estimates to be different, the agents estimates different weights for the two actions. All in all, if a client is characterized by $n$ morphological features, the estimation of value for the state-action pairs must include the update $4n$ weights. Formally,

\begin{align}
Q(S_t,a)&=\sum_{j}^{2}\sum_{i}^{n} f_{ij} w(a)_{ij} \\
Q(S_t,b)&=\sum_{j}^{2}\sum_{i}^{n} f_{ij} w(b)_{ij} 
\end{align}

\subsubsection{Action value estimation}


%\begin{thebibliography}{1}
%	
%\end{thebibliography}

\bibliography{Cleanerlearning}
\bibliographystyle{plain}

\end{document}