\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={The evolution of badges of status with learners},
            pdfauthor={Andrés Quiñones},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{The evolution of badges of status with learners}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Andrés Quiñones}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  
\usepackage{float}

\floatplacement{figure}{H}

\begin{document}
\maketitle

\hypertarget{the-hawk-dove-game}{%
\section{The Hawk-Dove game}\label{the-hawk-dove-game}}

Classical Hawk-Dove game. Individuals have one fo two genetically
determined phenotypic strategies. \emph{Hawks} are willing to start a
conflict over resources, while \emph{doves} prefer to stand down in the
hope to share the resource without an aggressive contest. Individuals
interact randomly with each other over their lifetimes and collect
resources depending on the strategy of theirs and their partners. In
contrast to the classical \emph{Hawk-dove}, in here when to \emph{hawks}
interact with each other one of them wins the contest and gets
\(V-\frac{C}{2}\) as payoff; while the loser gets \(-\frac{C}{2}\).
Where \(V\) is the value of the resource individuals are competing for,
and \(C\) is the cost of an agressive contest. individuals share the
cost, but only the winner gets the value of the resource.The rest of
payoff matrix follows the standard game.

Individuals may or may not vary in their quality. Quality is a numerical
value from 0 to 1, drawn from a truncated normal distribution when each
individual is born. Quality represents the different conditions under
which individuals might be raised, and may influence their competitive
abilities. Differences in quality between two individuals partly
determine who wins in a conflict when both players play hawk. The
probability of individual A wining a conflict over individual B is given
by

\begin{equation}
p(A)=\frac{1}{1+e^{-\beta(Q_A-Q_B)}},
\end{equation}

where \(Q_A\) and \(Q_b\) are the qualities of individuals A and B
respectively, and \(\beta\) is how important quality is in determining
who wins the contest. Individuals' reproductive success is proportional
to the payoff accumulated throughout their lifes. Figure
\ref{fig:HD_game} shows the evolutionary dynamics of the two genotypes,
which fit the game theoretical prediction.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//mutType_//basicHawkDove} \caption{\label{fig:HD_game}Hawk-dove game. Dashed line is the  game theoretical prediction for frequency of hawks.}\label{fig:fig1}
\end{figure}

\hypertarget{the-effect-of-learning}{%
\section{The effect of learning}\label{the-effect-of-learning}}

We let individuals have a \emph{badge of status} that potentially
signals their quality. To allow for this signalling, we let the size of
the \emph{badge of status} of an individual be dependent on her quality
through a reaction norm. we assume the size of the \emph{badge} is a
value from 0 to 1. This value is given by \begin{equation}
s_i = \frac{1}{1+e^{-(\alpha_i+\gamma_iQ_i)}}
\label{eq:react_norm}
\end{equation} The parameters \(\alpha\) and \(\gamma\) in equation
\ref{eq:react_norm} determine the shape of the reaction norm, and hence,
how honest the \emph{badge} is in signalling the quality of the
individual.Eventually, the aim is to allow the signalling system to
evolve, by letting the parameters of the reaction norm be inherited from
parent to offspring and be change by a mutational process.

\hypertarget{how-does-learning-work}{%
\subsection{How does learning work?}\label{how-does-learning-work}}

To complete the receiver side of the signalling system, we introduce a
new strategy in the game that we call \emph{learner}. \emph{Learners}
use a mixed strategy in the hawk-dove game. Which means, they play hawk
(dove) with a certain probability. Furthermore, the probability with
which the play each strategy changes during their lifetime according to
the payoff they recieve in previous interactions. In other words,
\emph{learners} learn to behave in the game. To implement the learning
process, we use the reinforcement learning formalism. Specifically, we
use the actor-critic algorithm. Individuals ( \emph{learners} ) perceive
the \emph{badge} of their counterpart as the environmental state in
which they find themselves. They estimate the value of that state and a
preference for playing hawk (dove) in that state. They update their
estimation and preference after each interaction they have. Given that
the badge size is a real number between 0 and 1, there are infinitely
many environmental states. Thus, for their value estimation and
preference computation, individual must use a form of generalization. We
let them use as generalization a function approximation method based on
radial basis functions. Each individual uses five \emph{centers} spread
uniformly throughout the range of badge size variation (from 0 to 1).
Each time an individual interacts with a conspecific the size of the
badge of the interacting partner triggers are response from each one of
the \emph{centers}. This response decreases in strength as the badge
size is further from the \emph{center}. The decrease follows a radial
basis function with gaussian shape. The sum of the responses given by
all the centers provide the value estimation and the preference for one
of the two pure strategies (hawk or dove). During the learning process
individuals update the response that each one of these centers triggers.
In the case of the preference for the strategies the numerical value
resulting from the sum of the responses is translated to a probability
of one of the two pure strategies through a logistic function. Figure
\ref{fig:learning_cartoonRBF} shows a made up example of how the
function approximation works for the actor (top panel - probability of
choosing dove) and the critic (bottom panel - expected payoff from
facing an individual with a given badge size).

\begin{figure}
\includegraphics[width=0.8\linewidth]{cartoonRBF} \caption{\label{fig:learning_cartoonRBF}Function approximation for the actor and the critic. Using random values for the responses of each center. Circles show the location of the centers and the maximum response they trigger, while the lines show the total response along the badge size continium.}\label{fig:fig2}
\end{figure}

\hypertarget{results-when-individuals-do-not-vary-in-their-quality}{%
\subsection{Results when individuals do NOT vary in their
quality}\label{results-when-individuals-do-not-vary-in-their-quality}}

In Figure \ref{fig:learning_invar} we show the phenotypic frequencies
and interaction-type frequencies in 4 generations of a population
composed of only \emph{learners} that do not vary in their quality. The
population is also monomorphic for the reaction norm parameters
(\(\alpha=0\) and \(\beta=0\)). With the values chosen the badge size
distribution is the same as the quality distribution. Thus, the four
generations respresent just replicates of the learning process. The
results show that under quality invariance, the learning algorithm
develops similar phenotypic frequencies as the evolutionary dynamics of
the pure strategies.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//hawkDoveLearn} \caption{\label{fig:learning_invar}Learning equilibrium when individuals do not vary in quality. Dashed line is the  game theoretical prediction for frequency of hawks.}\label{fig:fig3}
\end{figure}

\hypertarget{results-when-individuals-vary-in-their-quality}{%
\subsection{Results when individuals vary in their
quality}\label{results-when-individuals-vary-in-their-quality}}

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//hawkDoveLearn_0.1} \caption{\label{fig:learning_var0.1}Learning equilibrium when individuals vary in quality. Dashed lines are the  game theoretical predictions in the standard game.}\label{fig:fig4}
\end{figure}

\hypertarget{overal-effect-of-variance}{%
\subsection{Overal effect of variance}\label{overal-effect-of-variance}}

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//effectQualVariance} \caption{\label{fig:learning_all_var}Effect of variation in quality for the learning equilibrium.}\label{fig:fig5}
\end{figure}

\hypertarget{how-does-learning-look-like-in-the-hawk-dove-game}{%
\subsection{How does learning look like in the Hawk-Dove
game}\label{how-does-learning-look-like-in-the-hawk-dove-game}}

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//WeightsVarQualSt} \caption{\label{fig:AC_HD}Within population variation in the learning parameters at the end of learning for three scenarios of variation in the quality of individuals.}\label{fig:fig6}
\end{figure}

\hypertarget{how-do-the-learning-dynamics-look-like}{%
\subsection{How do the learning dynamics look
like}\label{how-do-the-learning-dynamics-look-like}}

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//mutType_//learnDyn200} \caption{\label{fig:learn_dyn}Learning dynamics of the individuals in a population.}\label{fig:fig7}
\end{figure}

\hypertarget{how-do-learners-fare-against-the-pure-strategies}{%
\section{How do learners fare against the ``pure''
strategies}\label{how-do-learners-fare-against-the-pure-strategies}}

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//mutType_//evolDynTypes} \caption{\label{fig:types_comp}Evolutionary dynamics of the learners competing against pure strategies.}\label{fig:fig8}
\end{figure}

\hypertarget{whats-next}{%
\section{What's next?}\label{whats-next}}

Let reaction norm evolve, under different initial conditions.


\end{document}
