\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={The evolution of badges of status with learners},
            pdfauthor={Andrés Quiñones},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{The evolution of badges of status with learners}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Andrés Quiñones}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
    \date{}
    \predate{}\postdate{}
  
\usepackage{float}

\begin{document}
\maketitle

\section{The Hawk-Dove game}\label{the-hawk-dove-game}

Individuals have one of two genetically determined phenotypic
strategies. \emph{Hawks} are willing to start a conflict over resources,
while \emph{doves} prefer to stand down in the hope to share the
resource without an aggressive contest. Individuals interact randomly
with each other over their lifetimes and collect resources depending on
the strategy of theirs and their partners. In contrast to the classical
\emph{Hawk-dove}, in here when two \emph{hawks} interact with each other
one of them wins the contest and gets \(V-\frac{C}{2}\) as payoff; while
the loser gets \(-\frac{C}{2}\). Where \(V\) is the value of the
resource individuals are competing for, and \(C\) is the cost of an
aggressive contest. individuals share the cost, but only the winner gets
the value of the resource.The rest of payoff matrix follows the standard
game.

Individuals may or may not vary in their quality. Quality is a numerical
value from 0 to 1, drawn from a truncated normal distribution when each
individual is born. Quality represents the different conditions under
which individuals might be raised, and may influence their competitive
abilities. Differences in quality between two individuals partly
determine who wins in a conflict when both players play hawk. The
probability of individual A wining a conflict over individual B is given
by

\begin{equation}
p(A)=\frac{1}{1+e^{-\beta(Q_A-Q_B)}},
\end{equation}

where \(Q_A\) and \(Q_b\) are the qualities of individuals A and B
respectively, and \(\beta\) is how important quality is in determining
who wins the contest. Individuals' reproductive success is proportional
to the payoff accumulated throughout their life. Figure
\ref{fig:HD_game} shows the evolutionary dynamics of the two genotypes,
which fit the game theoretical prediction.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations/mutType_/basicHawkDove} \caption{\label{fig:HD_game}Hawk-dove game. Dashed line is the  game theoretical prediction for frequency of hawks.}\label{fig:fig1}
\end{figure}

\section{The effect of learning}\label{the-effect-of-learning}

We let individuals have a \emph{badge of status} that potentially
signals their quality. To allow for this signalling, we let the size of
the \emph{badge of status} of an individual be dependent on her quality
through a reaction norm. we assume the size of the \emph{badge} is a
value from 0 to 1. This value is given by

\begin{equation}
s_i = \frac{1}{1+e^{-(\alpha_i+\gamma_iQ_i)}}
\label{eq:react_norm}
\end{equation}

The parameters \(\alpha\) and \(\gamma\) in equation \ref{eq:react_norm}
determine the shape of the reaction norm, and hence, how honest the
\emph{badge} is in signalling the quality of the individual.Eventually,
the aim is to allow the signalling system to evolve, by letting the
parameters of the reaction norm be inherited from parent to offspring
and be change by a mutational process.

\subsection{How does learning work?}\label{how-does-learning-work}

To complete the receiver side of the signalling system, we introduce a
new strategy in the game that we call \emph{learner}. \emph{Learners}
use a mixed strategy in the hawk-dove game. Which means, they play hawk
(dove) with a certain probability. Furthermore, the probability with
which the play each strategy changes during their lifetime according to
the payoff they receive in previous interactions. In other words,
\emph{learners} learn to behave in the game. To implement the learning
process, we use the reinforcement learning formalism. Specifically, we
use the actor-critic algorithm (Sutton and Barto 2018). Individuals (
\emph{learners} ) perceive the \emph{badge} of their counterpart as the
environmental state in which they find themselves. They estimate the
value of that state and a preference for playing hawk (dove) in that
state. They update their estimation and preference after each
interaction they have. Given that the badge size is a real number
between 0 and 1, there are infinitely many environmental states. Thus,
for their value estimation and preference computation, individual must
use a form of generalization. We let them use as generalization a
function approximation method based on radial basis functions. Each
individual uses five \emph{centers} spread uniformly throughout the
range of badge size variation (from 0 to 1). Each time an individual
interacts with a con-specific the size of the badge of the interacting
partner triggers a response from each one of the \emph{centers}. This
response decreases in strength as the badge size is further from the
\emph{center}. The decrease follows a radial basis function with
Gaussian shape. The sum of the responses given by all the centers
provide the value estimation and the preference for one of the two pure
strategies (hawk or dove). During the learning process individuals
update the \emph{weight} of the response that each one of these centers
triggers. In the case of the preference for the strategies the numerical
value resulting from the sum of the responses is translated to a
probability of one of the two pure strategies through a logistic
function. Figure \ref{fig:learning_cartoonRBF} shows a made up example
of how the function approximation works for the actor (top panel -
probability of choosing dove) and the critic (bottom panel - expected
payoff from facing an individual with a given badge size).

\begin{figure}
\includegraphics[width=0.8\linewidth]{cartoonRBF} \caption{\label{fig:learning_cartoonRBF}Function approximation for the actor and the critic. Using random values for the responses of each center. Circles show the location of the centers and the maximum response they trigger, while the lines show the total response along the badge size continium.}\label{fig:fig2}
\end{figure}

\subsection{Results when individuals do NOT vary in their
quality}\label{results-when-individuals-do-not-vary-in-their-quality}

In Figure \ref{fig:learning_invar} we show the phenotypic frequencies
and interaction-type frequencies in 4 generations of a population
composed of only \emph{learners} that do not vary in their quality. The
population is also monomorphic for the reaction norm parameters
(\(\alpha=0\) and \(\gamma=0\)). With the values chosen the badge size
distribution is the same as the quality distribution. Thus, the four
generations represent just replicates of the learning process. The
results show that under quality invariance, the learning algorithm
develops similar phenotypic frequencies as the evolutionary dynamics of
the pure strategies.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//hawkDoveLearn} \caption{\label{fig:learning_invar}Learning equilibrium when individuals do not vary in quality. Dashed line is the  game theoretical prediction for frequency of hawks.}\label{fig:fig3}
\end{figure}

\subsection{Results when individuals vary in their
quality}\label{results-when-individuals-vary-in-their-quality}

In figure \ref{fig:learning_var0.1} we show results of phenotypic and
interaction types frequencies from simulations in which individuals vary
in their quality. The value of quality is drawn from a truncated normal
distribution with mean 0.5 and standard deviation 0.1. Compared to the
expectation from evolutionary dynamics of the standard game, the
learning scenario with quality variation shows lower levels of hawks and
therefore lower levels of escalated conflict. At this point individuals
do not respond adaptively to the quality of their partner, instead their
preference for a given action is dependent on their own quality.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//hawkDoveLearn_0.1} \caption{\label{fig:learning_var0.1}Learning equilibrium when individuals vary in quality. Dashed lines are the  game theoretical predictions in the standard game.}\label{fig:fig4}
\end{figure}

\subsection{Overall effect of
variance}\label{overall-effect-of-variance}

In figure \ref{fig:learning_all_var} we show how the effect of
increasing variance in quality further decreases the amount of
individuals in a population expressing the hawk phenotype.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//QualStDv_//effectQualVariance} \caption{\label{fig:learning_all_var}Effect of variation in quality for the learning equilibrium.}\label{fig:fig5}
\end{figure}

\subsection{How does learning look like in the Hawk-Dove
game?}\label{how-does-learning-look-like-in-the-hawk-dove-game}

Figures \ref{fig:learning_var0.1} and \ref{fig:learning_all_var} only
show how learners behave at a population level, but they do not show the
inter-individual variation behind those frequencies. In figure
\ref{fig:AC_HD} we show the result of the function approximation
mechanism arising at the end of a generation for a sample of the
individuals of the populations. These results are shown under three
different levels of variation in quality. All three panes show that
through their learning processes individuals split up in two groups, one
which mostly behaves like a hawk and a second that mostly behaves like a
dove. The group of ``hawks'' is mostly, but no exclusively, composed of
individuals whose quality is on the upper half of the quality
distribution. In accordance, the group of ``doves'' is mostly composed
of individuals whose quality is on the lower half of the quality
distribution. The relation between quality and the preferred behavior of
individuals obviously only holds in scenarios with variance in quality.
In the scenario with quality invariance, the split of individuals in two
groups with a tendency for each behavior nevertheless remains.

\begin{figure}
\includegraphics[width=0.7\linewidth]{Simulations//QualStDv_//WeightsVarQualSt} \caption{\label{fig:AC_HD}Within population variation in the learning parameters at the end of learning for three scenarios of variation in the quality of individuals.}\label{fig:fig6}
\end{figure}

\subsection{How do the learning dynamics look
like}\label{how-do-the-learning-dynamics-look-like}

In figure \ref{fig:learn_dyn} we show the learning trajectory, for the
actor and the critic, of a sample of individuals. These dynamics show
how individuals split in the two groups according to their experiences.
Despite the robustness of the bi-modal distribution along the learning
dynamics, it is not uncommon that individual go from one group to the
other.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//mutType_//learnDyn200} \caption{\label{fig:learn_dyn}Learning dynamics of the individuals in a population.}\label{fig:fig7}
\end{figure}

\section{\texorpdfstring{How do learners fare against the ``pure''
strategies}{How do learners fare against the pure strategies}}\label{how-do-learners-fare-against-the-pure-strategies}

Finally, in a new set of simulations we explore the question of how
competitive learners are in competition with the pure strategies. Figure
\ref{fig:types_comp} show the results of competition among the three
strategies. The population starts with a 1:1 ratio of pure hawks and
doves, and learners are introduced by mutation. Learners manage to
increase in frequency. They, however, do not manage to outcompete the
pure strategies. In the end, there is a polymorphic equilibrium with the
three strategies coexisting, with learners being the most frequent of
the three.

\begin{figure}
\includegraphics[width=0.8\linewidth]{Simulations//mutType_//evolDynTypes} \caption{\label{fig:types_comp}Evolutionary dynamics of the learners competing against pure strategies.}\label{fig:fig8}
\end{figure}

\section{What's next?}\label{whats-next}

Let reaction norm evolve, under different initial conditions.

\section*{Cited literature}\label{cited-literature}
\addcontentsline{toc}{section}{Cited literature}

\hypertarget{refs}{}
\hypertarget{ref-sutton_reinforcement_2018}{}
Sutton, Richard S., and Andrew G. Barto. 2018. \emph{Reinforcement
Learning: An Introduction}. Edited by Francis Bach. Second edition
edition. Cambridge, MA: A Bradford Book.


\end{document}
