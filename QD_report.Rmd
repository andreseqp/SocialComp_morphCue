---
title: "The evolution of badges of status with learners"
author: "Andrés Quiñones"
output: 
  pdf_document: 
    fig_caption: yes
    keep_tex: yes
header-includes: \usepackage{float}
                \floatplacement{figure}{H}
---

# The Hawk-Dove game

Classical Hawk-Dove game. Individuals have one fo two genetically determined phenotypic strategies. _Hawks_ are willing to start a conflict over resources, while _doves_ prefer to stand down in the hope to share the resource without an aggressive contest. Individuals interact randomly with each other over their lifetimes and collect resources depending on the strategy of theirs and their partners. In contrast to the classical _Hawk-dove_, in here when to _hawks_ interact with each other one of them wins the contest and gets $V-\frac{C}{2}$ as payoff; while the loser gets $-\frac{C}{2}$. Where $V$ is the value of the resource individuals are competing for, and $C$ is the cost of an agressive contest. individuals share the cost, but only the winner gets the value of the resource.The rest of payoff matrix follows the standard game.

Individuals may or may not vary in their quality. Quality is a numerical value from 0 to 1, drawn from a truncated normal distribution when each individual is born. Quality represents the different conditions under which individuals might be raised, and may influence their competitive abilities. Differences in quality between two individuals partly determine who wins in a conflict when both players play hawk. The probability of individual A wining a conflict over individual B is given by

\begin{equation}
p(A)=\frac{1}{1+e^{-\beta(Q_A-Q_B)}},
\end{equation}

where $Q_A$ and $Q_b$ are the qualities of individuals A and B respectively, and $\beta$ is how important quality is in determining who wins the contest. Individuals' reproductive success is proportional to the payoff accumulated
throughout their lifes. Figure \ref{fig:HD_game} shows the evolutionary dynamics of  the two genotypes, which fit the game theoretical prediction.


```{r fig1, echo=FALSE, fig.cap="\\label{fig:HD_game}Hawk-dove game. Dashed line is the  game theoretical prediction for frequency of hawks.", out.width = '80%'}
knitr::include_graphics("Simulations//mutType_//basicHawkDove.png")
```


# The effect of learning 

We let individuals have a _badge of status_ that potentially signals their quality. To allow for this signalling, we let the size of the _badge of status_ of an individual be dependent on her quality through a reaction norm. we assume the size of the _badge_ is a value from 0 to 1. This value is given by
\begin{equation}
s_i = \frac{1}{1+e^{-(\alpha_i+\gamma_iQ_i)}}
\label{eq:react_norm}
\end{equation}
The parameters $\alpha$ and $\gamma$ in equation \ref{eq:react_norm} determine the
shape of the reaction norm, and hence, how honest the _badge_ is in signalling the
quality of the individual.Eventually, the aim is to allow the signalling system to
evolve, by letting the parameters of the reaction norm be inherited from parent to offspring and be change by a mutational process.

## How does learning work?

To complete the receiver side of the signalling system, we introduce a new strategy in the game that we call _learner_. _Learners_ use a mixed strategy in the hawk-dove game. Which means, they play hawk (dove) with a certain probability.
Furthermore, the probability with which the play each strategy changes during their lifetime according to the payoff they recieve in previous interactions. In other words, _learners_ learn to behave in the game. To implement the learning
process, we use the reinforcement learning formalism. Specifically, we use the actor-critic algorithm. Individuals ( _learners_ ) perceive the _badge_ of their counterpart as the environmental state in which they find themselves. They estimate the value of that state and a preference for playing hawk (dove) in that state. They update their estimation and preference after each interaction they have. Given that the badge size is a real number between 0 and 1, there are infinitely many environmental states. Thus, for their value estimation and preference computation, individual must use a form of generalization. We let them
use as generalization a function approximation method based on radial basis functions. Each individual uses five _centers_ spread uniformly throughout the range of badge size variation (from 0 to 1). Each time an individual interacts with a conspecific the size of the badge of the interacting partner triggers are response from each one of the _centers_. This response decreases in strength as the badge size is further from the _center_. The decrease follows a radial basis function with gaussian shape. The sum of the responses given by all the centers provide the value estimation and the preference for one of the two pure strategies (hawk or dove). During the learning process individuals update the response that each one of these centers triggers. In the case of the preference for the strategies the numerical value resulting from the sum of the responses is translated to a probability of one of the two pure strategies through a logistic function. Figure \ref{fig:learning_cartoonRBF} shows a made up example of how the function approximation works for the actor (top panel - probability of choosing dove) and the critic (bottom panel - expected payoff from facing an individual with a given badge size).

```{r fig2, echo=FALSE, fig.cap="\\label{fig:learning_cartoonRBF}Function approximation for the actor and the critic. Using random values for the responses of each center. Circles show the location of the centers and the maximum response they trigger, while the lines show the total response along the badge size continium.", out.width = '80%'}
knitr::include_graphics("cartoonRBF.png")
```


## Results when individuals do NOT vary in their quality

In Figure \ref{fig:learning_invar} we show the phenotypic frequencies and interaction-type frequencies in 4 generations of a population composed of only _learners_ that do not vary in their quality. The population is also monomorphic for the reaction norm parameters ($\alpha=0$ and $\gamma=0$). With the values chosen the badge size distribution is the same as the quality distribution. Thus, the four generations respresent just replicates of the learning process. The results show that under quality invariance, the learning algorithm develops similar phenotypic frequencies as the evolutionary dynamics of the pure strategies.

```{r fig3, echo=FALSE, fig.cap="\\label{fig:learning_invar}Learning equilibrium when individuals do not vary in quality. Dashed line is the  game theoretical prediction for frequency of hawks.", out.width = '80%'}
knitr::include_graphics("Simulations//QualStDv_//hawkDoveLearn.png")
```


## Results when individuals vary in their quality

```{r fig4, echo=FALSE, fig.cap="\\label{fig:learning_var0.1}Learning equilibrium when individuals vary in quality. Dashed lines are the  game theoretical predictions in the standard game.", out.width = '80%'}
knitr::include_graphics("Simulations//QualStDv_//hawkDoveLearn_0.1.png")
```

## Overal effect of variance


```{r fig5, echo=FALSE, fig.cap="\\label{fig:learning_all_var}Effect of variation in quality for the learning equilibrium.", out.width = '80%'}
knitr::include_graphics("Simulations//QualStDv_//effectQualVariance.png")
```


## How does learning look like in the Hawk-Dove game

```{r fig6, echo=FALSE, fig.cap="\\label{fig:AC_HD}Within population variation in the learning parameters at the end of learning for three scenarios of variation in the quality of individuals.", out.width = '80%'}
knitr::include_graphics("Simulations//QualStDv_//WeightsVarQualSt.png")
```

## How do the learning dynamics look like

```{r fig7, echo=FALSE, fig.cap="\\label{fig:learn_dyn}Learning dynamics of the individuals in a population.", out.width = '80%'}
knitr::include_graphics("Simulations//mutType_//learnDyn200.png")
```


# How do learners fare against the "pure" strategies

```{r fig8, echo=FALSE, fig.cap="\\label{fig:types_comp}Evolutionary dynamics of the learners competing against pure strategies.", out.width = '80%'}
knitr::include_graphics("Simulations//mutType_//evolDynTypes.png")
```



# What's next?

Let reaction norm evolve, under different initial conditions. 

