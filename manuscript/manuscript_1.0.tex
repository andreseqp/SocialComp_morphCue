% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfauthor={Andrés E. Quiñones; C. Daniel Cadena*; Redouan Bshary},
  pdfkeywords={My keywords},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
  \usepackage{float}
  \usepackage{setspace}\doublespacing
  \usepackage[backref=page]{hyperref}
  \linespread{2}
  \usepackage{lineno}
  \linenumbers
  \floatplacement{figure}{H}
 \newcommand{\beginsupplement}{ \setcounter{table}{0}     \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Learning can mediate both badges of status and individual
recognition}
\author{Andrés E. Quiñones\footnote{Laboratorio de Biología Evolutiva de
  Vertebrados, Departamento de Ciencias Biológicas, Universidad de los
  Andes, Bogotá, Colombia} \and C. Daniel Cadena* \and Redouan
Bshary\footnote{Institute of Biology, University of Neuchâtel,
  Neuchâtel, Switzerland}}
\date{July 18, 2022}

\begin{document}
\maketitle
\begin{abstract}
Adaptive behavioural responses often depend on qualities of the
interacting partner of an individual. For example, when competing for
resources, an individual might be better off escalating fights with
individuals of lower quality, while restraining from fighting
individuals of higher quality. Communication systems involving signals
of quality allow individuals to reduce uncertainty regarding the
fighting ability of their partners and make more adaptive behavioral
decisions. However, dishonest individuals can destabilize such
communications systems. One open question is whether cognitive
mechanisms, such as learning, can maintain the honesty of signals, thus,
favoring their evolutionary stability. Here we present evolutionary
simulations, where individuals can produce a signal proportional to
their quality; and learn along their lifetime the best response to the
signal emitted by their peers. In these simulations, learning on the
receiver side can mediate the evolution of signals of quality on the
sender side. When the cost of the signal is proportional to the quality
of the sender populations are only composed of honest signalers. When
the cost is not proportional to the quality of the signaler, the
population is composed of both honest and dishonest signalers. We argue
that learning can be a general cognitive mechanism playing a role in a
wide range of communication systems.
\end{abstract}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

The outcome of interactions with con-specifics is an important
determinant of fitness in social animals. Irrespective of whether such
interactions are cooperative or competitive, the actions of interacting
individuals partly determine their reproductive success. However, the
best action for an individual might vary depending on its condition, and
that of individuals with whom it interacts. Thus, acting based on
information about interacting partners is typically adaptive (Quiñones
et al. 2016). Acquiring this information, however, is far from trivial.
In some cases, interacting partners (signallers) might be `willing' to
provide accurate information, but in others it might be in their own
interest to conceal information (Johnstone 1997), or to provide
erroneous information (Johnstone and Norris 1993). Typically, in a given
context, some proportion of individuals will benefit from broadcasting
accurate information, whereas others will benefit from hiding it. Take,
for instance, an interaction between two individuals where one can help
the other. Given some costs and benefits, a potential donor will be
interested in helping related individuals. Therefore, for a relative of
the donor, broadcasting kinship is advantageous, whereas for an
unrelated individual, concealing the lack of kinship is better. Similar
scenarios may apply to many aspects of social life such as finding
mates, feeding offspring, dominance relationships and aggressive
interactions (Bradbury and Vehrencamp 2011; Møller 1988; Tibbetts and
Dale 2004). In dominance and aggressive interactions, an important piece
of information to guide individual actions is interacting partners
fighting ability, which is sometimes referred to as Resource Holding
Potential (RHP) (Parker 1974), or simply as quality. Responsiveness to
the quality of the partner is central to communication systems such as
badges of status, where an arbitrary signal conveys quality (Johnstone
and Norris 1993; Rohwer 1975). Here, arbitrary refers to a signal which
is not ontogenetically correlated with quality. Badges of status can be
evolutionarily stable whenever the signal imposes a fitness cost which
decreases with the quality of an individual (Botero et al. 2010;
Johnstone and Norris 1993), such that it's no longer in the interest of
low quality individuals to fake quality by producing a large badge. The
cost of honest signals is a more general principle of communications
systems, usually referred to as the handicap principle (Grafen 1990;
Zahavi 1975).

A somewhat ignored component of communication systems, in the context of
aggressive interactions, is the cognitive aspects of the receiver
module. Theoretical models often assume that Communication systems
relying on badges of status to have reaction norms as their mechanistic
underpinning (Botero et al. 2010), with individuals using their own
quality and the opponent's badge to determine whether to aggressively
engage in contest. Reaction norms allow individuals to respond to the
available information without a big cognitive burden. That is at least
in contrast to other information processing mechanisms like individual
recognition. With individual recognition individuals associate cue of
their peers to their quality. These association must be learn thoughout
individuals lifes, thus it has the usual cognitive requirements of an
associative learning process. An alternative view of badges of status is
that individuals learn to react to them based on their experiences
(Guilford and Dawkins 1991), which would imply that receivers must learn
to associate signals with the fighting ability of bearers just as in
systems based on individual recognition. In cases where badges of status
vary quantitatively (e.g.~in size or intensity), fighting ability may
increase monotonically with attributes of the badge and would be
reinforced by every interaction, so in principle learning would be
faster than in systems involving individual recognition in which the
association between signals and their meaning would vary depending on
the interacting partners. In any case, learning could be a central
cognitive mechanism in both types of communication systems, but the role
of learning in these contexts has not been thoroughly explored in the
empirical nor theoretical literature.

Associative learning is a key cognitive mechanism that allows
individuals to associate rewards with environmental stimuli and thus
behave adaptively (Staddon 2016). Associative learning exists in all
major vertebrate taxa, and in many invertebrates as well (Heyes 2012;
Macphail 1982; Staddon 2016; Behrens et al. 2008). Theory has shown that
natural selection favours these associations in complex environments
where conditions are hard to predict (Dridi and Lehmann 2016). Besides
its wide taxonomic and ecological relevance, associative learning is a
flexible cognitive mechanism whose underpinnings show interspecific
variation (Enquist, Lind, and Ghirlanda 2016; Quiñones et al. 2019), so
presumably, it has been modified by natural selection. Despite all this,
associative learning is not often included in the narrative of
evolutionary explanations of behavioural patterns (Fawcett, Hamblin, and
Giraldeau 2013; Kamil 1983; McAuliffe and Thornton 2015). Computational
models of evolution can play an important role to overcome the lack of
integration between learning and evolution. Reinforcement learning
theory encompasses a series of computational methods inspired on the
psychological and neurological mechanisms of associative learning
(Sutton and Barto 2018). These set of algorithms allow the
implementation of biologically realistic problems, capturing the essence
of learning processes (Frankenhuis, Panchanathan, and Barto 2018;
Quiñones et al. 2019). Furthermore, these algorithms can be embedded in
evolutionary simulations to generate theoretical predictions of the
effect that learning can have in behavioural evolution (Leimar and
McNamara 2019).

In here we present an evolutionary model where individuals use
associative learning to develop a tendency to behave aggressively (or
peacefully), in the context of competition over resources, depending on
the quantitative morphological trait they perceive in their opponent
(badge). Over evolutionary time individuals evolve the size of their
badge, and whether it depends on their quality. Under this simple set
up, individuals can use the badge as a signal of quality. We use the
model to assess under what conditions of interaction structure we expect
different communication signals to evolve.

\hypertarget{the-model}{%
\subsection{The model}\label{the-model}}

We model the evolution of signals of quality in the context of agonistic
interactions. For simplicity we consider a population of haploid
individuals with non-overlapping generations. Individuals are born every
generation with a level of quality (\(Q_i\), where \(i\) is subscript of
the population vector of size \(N\)) given by a number between zero and
one, which is drawn from a truncated normal distribution
\(N(\mu = 0.5,\sigma)\). An individual with quality \(0\) has the lowest
RHP, while an individual with quality \(1\) has the highest. As part of
development processes individuals produce a morphological signal
(badge), the size of such depends on their level of quality according to
a reaction norm given by \(B_i=1/(1+e^{\alpha_i-\beta_i Q_i})\); where
are \(\alpha_i\) and \(\beta_i\) are individual specific traits that
determine the shape of the reaction norm (Fig. \ref{fig:model-struc} A).
Variation in \(\alpha_i\) and \(\beta_i\) means individuals can have
either uninformative (flat) or informative (logistic) norms. The size of
the signal is constrained between zero and one. We assume different
values of these traits are given by different alleles, and are inherited
from mother to offspring. Unless, with a small probability, mutation
changes the allelic value of the offspring. After birth individuals go
through a round of viability selection. Individual specific survival
probability is given by \(s_i=1/(1+e^{-k_1-k_2(Q_i-B_i)})\). Where
\(k_1\) and \(k_2\) are parameter values determining the shape of the
survival function. Importantly, if \(k_2=0\) survival probability is
independent of quality, while if \(k_2>0\) lower quality individuals pay
a higher price for similar-sized badge, fulfilling the core assumption
of the handicap principle (Botero et al. 2010; Grafen 1990; Johnstone
and Norris 1993).

Individuals who survive engage in a series of pairwise interactions
where they compete for resources. Individuals in real populations
typically do not interact at random nor with all individuals in the
population (Kurvers et al. 2014). In order to allow for non-random
interactions, we use the population vector to define an interaction
neighbourhood for the focal individual, the size of the neighbourhood
(\(g\)) is the number of positions around the focal's from which the
partner is drawn. So, if \(g=N\) interactions are global; as \(g\) gets
smaller interactions are more local. In an interaction each individual
must decide whether to escalate a fight or not. Following the classic
\emph{hawk-dove} game (Maynard-Smith 1982), if the focal individual
fights and its partner does not, the focal gets as pay-off the resource
of value \(V\) while its partner gets nothing; if both individuals
restrain from fighting they split up the resource in half; if both
decide to fight they split up the cost of fighting and the winner takes
over the resource. We further assume that the probability of wining a
fight for the focal individual depends on the difference in quality
between her and her interacting partner; specifically it is given by
\(p_{ij}=1/(1+e^{-k_3(Q_i-Q_j)})\), where \(k_3\) is a parameter
defining how strong the quality difference determines the winning
probability; and \(i\) and \(j\) denote the the position in the
population vector of the focal and its interacting partner respectively.

The decision of whether to escalate a fight against an interacting
partner can be dependent on the size of the partner's badge, the actual
dependency is determined by the focal´s experiences through a learning
process. We implement learning using the actor-critic approach from
reinforcement learning theory {[}RL; Sutton and Barto (2018);Quiñones et
al. (2019); Leimar and McNamara (2019){]}. Individuals estimate the
reward (pay-off) expected from interacting with partners of different
badge sizes (the critic in RL terminology). After every interaction they
update the estimate of reward proportionally to the difference between
their current estimate and the observed reward (prediction error
\(\delta\)) and to the speed of learning (\(\mathrm{A}\)). Furthermore,
they express different probabilities of retreating (attacking) depending
on the badge size of their opponent (the actor in RL). They update the
probability of retreating (attacking) up or down depending on whether
retreating (attacking) leads to an increase in the reward estimation.
So, if a focal individual decides to escalate a fight against an
individual with small badge, and that leads to an increase in the reward
estimation, the focal individual will increase the probability of
escalating fights with individuals of small badges in the future. Given
that badge size is a real number between 0 and 1, there are infinitely
many badge sizes. Thus, the reward estimation, as well as the
probability of retreating (escalating), must be generalized across
different values. To implement generalization we use the linear function
approximation method based on radial basis functions (Sutton and Barto
2018). Specifically, we pick \(c\) feature centres, which are evenly
spaced values along the badge size interval ({[}0,1{]}). Each one of
these features centres is associated with a weight for reward estimation
and tendency to play retreat in a given interaction. The reward
estimation and the tendency to retreat are calculated (in every
interaction) as the sum of the weights associated with each feature,
weighted by the response triggered by the feature (Fig.
\ref{fig:model-struc} B, dots represent the feature weights) The
response of each feature centre diminishes as a Gaussian function with
the distance between the feature centre and the badge size of the
partner (Fig. \ref{fig:model-struc} B, grey line). Formally the reward
estimation \(\hat{R}\) when the focal individual (\(i\)) faces
individual \(j\) is given by,\\
\begin{equation}
\hat{R}_{ij} =  \sum_{z=1}^{c} x_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where \(x_z\) is the weight of feature centre \(z\) on the reward
estimation; and \(\theta\) is the width of the generalization function.
Similarly, the tendency to retreat is given by the sum of feature
weights associated with the actor, and the probability is obtained by
applying a logistic transformation (Fig. \ref{fig:model-struc} B, black
line). So formally, the log-odds to retreat when facing individual \(j\)
is

\begin{equation}
logit(q_{ij}) =  \sum_{z=1}^{c} y_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where \(y_z\) is the weight of feature centre \(z\) on the tendency to
retreat.

After all their interactions, individuals in the population reproduce
proportionally to their fitness \(w_i\), which is a sum of the baseline
fitness (\(w_0\)) and all the pay-offs obtained throughout their life.
Thus, the combination of natural selection and genetic drift changes the
distribution of values in \(\alpha\) and \(\beta\) that segregates in
the population, effectively changing the badges expressed and the
communication system.

\begin{figure}

{\centering \includegraphics{manuscript_1.0_files/figure-latex/model-struc-1} 

}

\caption{Model of communication in the context of aggressive interactions. In A the sender code: a reaction norm determines how the badge size is determined by the quality of the individual. Red shows an informative reaction norm, while the blue shows a uninformative reaction norm. B the receiver code: individuals have a behavioral reaction norm that determines their probability of retreating (black line). The reaction norm arises from generalizing the information from the feature weights (black dots). Generalization is represented by the grey line and its axis, which shows the response triggered by the fourth feature diminishes as the value evaluated is further from the feature center. The learning process moves the feature weights (black dots) up or down depending on whether that leads to an increase in the estimated reward. In C and D, effect of learning on the receiver strategy. Receivers in C face signallers with uninformative reaction norms (blue line in A), while in D they face signallers with informative reaction norms (red line in A). Colour scale in C (applies also for D) indicates the quality of the individual}\label{fig:model-struc}
\end{figure}

\hypertarget{results}{%
\subsection{Results}\label{results}}

We first present the outcome of simulations where we prevent the
evolution of the sender code (by setting the mutation rate to \(0\)),
and assume that all individuals in the population display either an
uninformative or informative badge size with respect to their quality.
These simulations show what type of receiver strategy develops through a
learning process. Figure \ref{fig:model-struc} (C and D) shows the
receiver strategy developed through learning; panel C is for receivers
that faced uninformative signals, and D informative ones. When learners
face uninformative signals, they change their probability of retreating
depending on their own quality. Individuals with higher quality (red
tones), after the learning process, have a high probability of
attacking; while individuals of lower quality (blue tones) mostly
retreat from confrontations. Thus, learning splits the population of
receivers into the two classic pure strategies of hawks and doves. Given
that we have a assumed a monomorphic population with unresponsive
reaction norms on the signalling side, the changes triggered by learning
only affect a small range of badge sizes. In contrast, when receivers
face informative reaction norms on the side of the signaller (panel B in
fig.~\ref{fig:model-struc}), receivers use the badge size of their
interacting partners to determine whether to retreat or attack. The
relationship is given by a threshold-like reaction norm, where the
change from retreating to attacking depends on the quality of the
receiver. As expected, the higher the quality of the receiver the larger
the badge size that triggers a retreat.

\begin{verbatim}
## Warning in knitr::include_graphics(here("Simulations", "betCostEvol1_", :
## It is highly recommended to use relative paths for images. You had
## absolute paths: "M:/Projects/SocialComp_morphCue/Simulations/betCostEvol1_/
## evolDyn0_betCost5.png"
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{M:/Projects/SocialComp_morphCue/Simulations/betCostEvol1_/evolDyn0_betCost5} 

}

\caption{Evolution of the badge size as a handicap mediated by learning. Middle panels show the evolutionary dynamics of the sender code. On the left, changes in the distribution of values for the intercept of the reaction norm ($\alpha$); on the right changes in the distribution of the slope ($\beta$). Dashed lines in both panels show the mean of the distribution. Grey lines show the generation time corresponding to the panels above and below showing the sender and receiver code. In the panels above, the reaction norms correspond to individuals in the population after the interaction round. Color scale indicates quality just as in Fig. \ref{fig:model-struc}. In the panels below, colors represent a cluster clasification performed by the k-means algorithm}\label{fig:handicap}
\end{figure}

When we allow the evolution of the sender code (\(\alpha\) and \(\beta\)
change subject to natural selection and genetic drift), and the badge
size work as a handicap (i.e.~the cost of the badge is inversely
proportional to the quality of the individual), the sender code evolves
to produce an honest signal Fig \ref{fig:handicap}). The evolution of
the sender code does not happen immediately after the start of the
evolutionary process. Instead, the evolutionary dynamics of the reaction
norm parameters (Fig. \ref{fig:handicap}, X) show a set of steps. First,
the intercept of the reaction norm (\(\alpha\)) evolves to higher
values, reducing the average badge size in the populations. That makes
sense because reaction norms segregating in the population are flat at
this point, and consequently the receiver code does not respond to the
badge size, bigger badges are costlier and do not trigger lower attack
probabilities. During those first generations, the slope of the sender
reaction norm (\(\beta\)) remains around the initial value of zero. At
around generation 4000, the slope evolves toward positive values. Which
implies that larger badges correlate with higher quality. Receivers
learn to react to such correlation, reducing the probability of attack
towards individual of larger badges. Hence, natural selection favours
larger values of \(\beta\), eventually leading to an evolutionary
equilibrium where badge size is an honest signal of quality mediated by
the learned responses of receivers. The evolutionary trajectory
portrayed in figure \ref{fig:handicap} is not the only possible
trajectory. If the slope of the sender reaction norms (\(\beta\))
evolves toward negative values before badges become handicaps, receivers
never learn to react to the size of badges. Thus, badges are only costly
and do not provide information about the quality of an individual(Fig.
\ref{fig:noBadge} . Eventually, the badge disappears from the
population.

When production of the badge is not costly (i.e.~does not work as a
handicap), the parameters of the sender code go through evolutionary
branching event, triggering the evolution of different types of signals
in the population. Fig. \ref{fig:branching}A and B shows changes in the
distribution of the sender reaction norm (\(\alpha\) and \(\beta\))
along evolutionary time. Darker areas show trait values with high
frequency in the population. Populations start monomorphic with a value
of zero on both traits, and mutations quickly build up a normal
distribution around the starting value. Within the first 2000
generations the unimodal distribution splits into a bimodal one. In the
case of \(\alpha\), one of the peaks splits further, so at the end of
the evolutionary simulation the distribution of \(\alpha\) value in the
population shows three peaks. In the case of \(\beta\), the two initial
peaks coexist for around 4000 generations. Later on one of the types
goes extinct and the distribution goes back to being unimodal. These
changes in the distribution of the sender reaction norm parameters imply
that during the second half of the simulation individuals can be
classified into three distinct types. Two types express a flat reaction
norm, meaning that their badge size is not informative of their quality,
while the third type shows intermediate badge sizes which are determined
by the quality of the individual (Fig. \ref{fig:branching}).
Furthermore, the receiver reaction norms develop through learning,
particularly those of individuals of intermediate quality, respond to
the signal of their sender type by increasing the probability of
retreating from a fight with individuals with larger badges (Fig.
\ref{fig:branching}).

\begin{verbatim}
## Warning in knitr::include_graphics(here("Simulations", "nIntGroupNormQual_", :
## It is highly recommended to use relative paths for images. You had absolute
## paths: "M:/Projects/SocialComp_morphCue/Simulations/nIntGroupNormQual_/
## evolDyn7_nIntGroup2000.png"
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{M:/Projects/SocialComp_morphCue/Simulations/nIntGroupNormQual_/evolDyn7_nIntGroup2000} 

}

\caption{The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \ref{fig:handicap}. The middle panels show changes in the distribution of values fir $\alpha$ and $\beta$ along evolutionary time. Panels above and bellow correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels.}\label{fig:branching}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{../Simulations/initAct_/indVarScatter_} 

}

\caption{WHy would you want to be recognized}\label{fig:startCond}
\end{figure}

\hypertarget{supplementary-material}{%
\section{Supplementary material}\label{supplementary-material}}

\beginsupplement

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{M:/Projects/SocialComp_morphCue/Simulations/betCostEvol1_/evolDyn5_betCost5} 

}

\caption{The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \ref{fig:handicap}. The middle panels show changes in the distribution of values fir $\alpha$ and $\beta$ along evolutionary time. Panels above and bellow correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels.}\label{fig:noBadge}
\end{figure}

\hypertarget{references}{%
\subsection*{References}\label{references}}
\addcontentsline{toc}{subsection}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-behrens_Associative_2008}{}}%
Behrens, Timothy E. J., Laurence T. Hunt, Mark W. Woolrich, and Matthew
F. S. Rushworth. 2008. {``Associative Learning of Social Value.''}
\emph{Nature} 456 (7219): 245--49.
\url{https://doi.org/10.1038/nature07538}.

\leavevmode\vadjust pre{\hypertarget{ref-botero_Evolution_2010}{}}%
Botero, C. A, I. Pen, J. Komdeur, and F. J Weissing. 2010. {``The
Evolution of Individual Variation in Communication Strategies.''}
\emph{Evolution} 64 (11): 3123--33.

\leavevmode\vadjust pre{\hypertarget{ref-bradbury_Principles_2011}{}}%
Bradbury, Jack W., and Sandra L. Vehrencamp. 2011. \emph{Principles of
{Animal Communication}}. 2nd Edition. {Sunderland, Mass}: {Sinauer
Associates is an imprint of Oxford University Press}.

\leavevmode\vadjust pre{\hypertarget{ref-dridi_Environmental_2016}{}}%
Dridi, Slimane, and Laurent Lehmann. 2016. {``Environmental Complexity
Favors the Evolution of Learning.''} \emph{Behav Ecol} 27 (3): 842--50.
\url{https://doi.org/10.1093/beheco/arv184}.

\leavevmode\vadjust pre{\hypertarget{ref-enquist_Power_2016}{}}%
Enquist, Magnus, Johan Lind, and Stefano Ghirlanda. 2016. {``The Power
of Associative Learning and the Ontogeny of Optimal Behaviour.''}
\emph{Royal Society Open Science} 3 (11): 160734.
\url{https://doi.org/10.1098/rsos.160734}.

\leavevmode\vadjust pre{\hypertarget{ref-fawcett_Exposing_2013}{}}%
Fawcett, Tim W., Steven Hamblin, and Luc-Alain Giraldeau. 2013.
{``Exposing the Behavioral Gambit: The Evolution of Learning and
Decision Rules.''} \emph{Behavioral Ecology} 24 (1): 2--11.
\url{https://doi.org/10.1093/beheco/ars085}.

\leavevmode\vadjust pre{\hypertarget{ref-frankenhuis_Enriching_2018}{}}%
Frankenhuis, Willem E., Karthik Panchanathan, and Andrew G. Barto. 2018.
{``Enriching Behavioral Ecology with Reinforcement Learning Methods.''}
\emph{Behavioural Processes}, February.
\url{https://doi.org/10.1016/j.beproc.2018.01.008}.

\leavevmode\vadjust pre{\hypertarget{ref-grafen_Biological_1990}{}}%
Grafen, Alan. 1990. {``Biological Signals as Handicaps.''} \emph{Journal
of Theoretical Biology} 144 (4): 517--46.
\url{https://doi.org/10.1016/S0022-5193(05)80088-8}.

\leavevmode\vadjust pre{\hypertarget{ref-guilford_Receiver_1991}{}}%
Guilford, Tim, and Marian Stamp Dawkins. 1991. {``Receiver Psychology
and the Evolution of Animal Signals.''} \emph{Animal Behaviour} 42 (1):
1--14. \url{https://doi.org/10.1016/S0003-3472(05)80600-1}.

\leavevmode\vadjust pre{\hypertarget{ref-heyes_Simple_2012}{}}%
Heyes, Cecilia. 2012. {``Simple Minds: A Qualified Defence of
Associative Learning.''} \emph{Phil. Trans. R. Soc. B} 367 (1603):
2695--2703. \url{https://doi.org/10.1098/rstb.2012.0217}.

\leavevmode\vadjust pre{\hypertarget{ref-johnstone_Recognition_1997}{}}%
Johnstone, Rufus A. 1997. {``Recognition and the Evolution of
Distinctive Signatures: When Does It Pay to Reveal Identity?''}
\emph{Proceedings of the Royal Society of London. Series B: Biological
Sciences} 264 (1387): 1547--53.
\url{https://doi.org/10.1098/rspb.1997.0215}.

\leavevmode\vadjust pre{\hypertarget{ref-johnstone_Badges_1993}{}}%
Johnstone, Rufus A., and Ken Norris. 1993. {``Badges of Status and the
Cost of Aggression.''} \emph{Behav Ecol Sociobiol} 32 (2): 127--34.
\url{https://doi.org/10.1007/BF00164045}.

\leavevmode\vadjust pre{\hypertarget{ref-kamil_Optimal_1983}{}}%
Kamil, Alan C. 1983. {``Optimal {Foraging Theory} and the {Psychology}
of {Learning}.''} \emph{Integr Comp Biol} 23 (2): 291--302.
\url{https://doi.org/10.1093/icb/23.2.291}.

\leavevmode\vadjust pre{\hypertarget{ref-kurvers_Evolutionary_2014}{}}%
Kurvers, Ralf H. J. M., Jens Krause, Darren P. Croft, Alexander D. M.
Wilson, and Max Wolf. 2014. {``The Evolutionary and Ecological
Consequences of Animal Social Networks: Emerging Issues.''} \emph{Trends
in Ecology \& Evolution} 29 (6): 326--35.
\url{https://doi.org/10.1016/j.tree.2014.04.002}.

\leavevmode\vadjust pre{\hypertarget{ref-leimar_Learning_2019}{}}%
Leimar, Olof, and John M. McNamara. 2019. {``Learning Leads to Bounded
Rationality and the Evolution of Cognitive Bias in Public Goods
Games.''} \emph{Scientific Reports} 9 (1): 1--9.
\url{https://doi.org/10.1038/s41598-019-52781-7}.

\leavevmode\vadjust pre{\hypertarget{ref-macphail_Brain_1982}{}}%
Macphail, Euan M. 1982. \emph{Brain and {Intelligence} in
{Vertebrates}}. {Clarendon Press}.

\leavevmode\vadjust pre{\hypertarget{ref-maynard-smith_Evolution_1982}{}}%
Maynard-Smith, John. 1982. \emph{Evolution and the {Theory} of {Games}}.
1 edition. {Cambridge ; New York}: {Cambridge University Press}.

\leavevmode\vadjust pre{\hypertarget{ref-mcauliffe_Psychology_2015}{}}%
McAuliffe, K., and A. Thornton. 2015. {``The Psychology of Cooperation
in Animals: An Ecological Approach.''} \emph{J Zool} 295 (1): 23--35.
\url{https://doi.org/10.1111/jzo.12204}.

\leavevmode\vadjust pre{\hypertarget{ref-moller_Female_1988}{}}%
Møller, Anders Pape. 1988. {``Female Choice Selects for Male Sexual Tail
Ornaments in the Monogamous Swallow.''} \emph{Nature} 332 (6165):
640--42. \url{https://doi.org/10.1038/332640a0}.

\leavevmode\vadjust pre{\hypertarget{ref-parker_Assessment_1974}{}}%
Parker, G. A. 1974. {``Assessment Strategy and the Evolution of Fighting
Behaviour.''} \emph{Journal of Theoretical Biology} 47 (1): 223--43.
\url{https://doi.org/10.1016/0022-5193(74)90111-8}.

\leavevmode\vadjust pre{\hypertarget{ref-quinones_Negotiation_2016}{}}%
Quiñones, Andrés E., G. Sander van Doorn, Ido Pen, Franz J. Weissing,
and Michael Taborsky. 2016. {``Negotiation and Appeasement Can Be More
Effective Drivers of Sociality Than Kin Selection.''} \emph{Phil. Trans.
R. Soc. B} 371 (1687): 20150089.
\url{https://doi.org/10.1098/rstb.2015.0089}.

\leavevmode\vadjust pre{\hypertarget{ref-quinones_Reinforcement_2019}{}}%
Quiñones, Andrés E., Olof Leimar, Arnon Lotem, and Redouan Bshary. 2019.
{``Reinforcement {Learning Theory Reveals} the {Cognitive Requirements}
for {Solving} the {Cleaner Fish Market Task}.''} \emph{The American
Naturalist}, December, 000--000. \url{https://doi.org/10.1086/707519}.

\leavevmode\vadjust pre{\hypertarget{ref-rohwer_Social_1975a}{}}%
Rohwer, Sievert. 1975. {``The {Social Significance} of {Avian Winter
Plumage Variability}.''} \emph{Evolution} 29 (4): 593--610.
\url{https://doi.org/10.2307/2407071}.

\leavevmode\vadjust pre{\hypertarget{ref-staddon_Adaptive_2016}{}}%
Staddon, J. E. R. 2016. \emph{Adaptive {Behavior} and {Learning}}. 2
edition. {Cambridge}: {Cambridge University Press}.

\leavevmode\vadjust pre{\hypertarget{ref-sutton_Reinforcement_2018}{}}%
Sutton, Richard S., and Andrew G. Barto. 2018. \emph{Reinforcement
{Learning}: {An Introduction}}. Edited by Francis Bach. Second edition
edition. {Cambridge, MA}: {A Bradford Book}.

\leavevmode\vadjust pre{\hypertarget{ref-tibbetts_Socially_2004}{}}%
Tibbetts, Elizabeth A., and James Dale. 2004. {``A Socially Enforced
Signal of Quality in a Paper Wasp.''} \emph{Nature} 432 (7014): 218--22.
\url{https://doi.org/10.1038/nature02949}.

\leavevmode\vadjust pre{\hypertarget{ref-zahavi_Mate_1975}{}}%
Zahavi, Amotz. 1975. {``Mate Selection\textemdash -{A} Selection for a
Handicap.''} \emph{Journal of Theoretical Biology} 53 (1): 205--14.
\url{https://doi.org/10.1016/0022-5193(75)90111-3}.

\end{CSLReferences}

\end{document}
