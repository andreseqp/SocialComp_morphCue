---
title: |
  | Learning can mediate both badges of status and individual recognition
author:
- Andrés E. Quiñones^[Laboratorio de Biología Evolutiva de Vertebrados, Departamento
  de Ciencias Biológicas, Universidad de los Andes, Bogotá, Colombia]
- C. Daniel Cadena*
- Redouan Bshary^[Institute of Biology, University of Neuchâtel, Neuchâtel, Switzerland]
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  word_document: default
fig_caption: yes
fontsize: 12pt
header-includes: |
  ```{=latex}
    \usepackage{float}
    \usepackage{setspace}\doublespacing
    \usepackage[backref=page]{hyperref}
    \linespread{2}
    \usepackage{lineno}
    \linenumbers
    \floatplacement{figure}{H}
   \newcommand{\beginsupplement}{ \setcounter{table}{0}     \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
  ```
keep_tex: yes
keywords: My keywords
linespread: 2
bibliography: ../SocialMorphCue.bib
spacing: double
style: C:\Users\andre\Zotero\styles\elife.csl
abstract: | 
  Adaptive behavioural responses often depend on qualities of the interacting partner of an individual. For example, when competing for resources, an individual might be better off escalating fights with individuals of lower quality, while restraining from fighting individuals of higher quality. Communication systems involving signals of quality allow individuals to reduce uncertainty regarding the fighting ability of their partners and make more adaptive behavioral decisions. However, dishonest individuals can destabilize such communications systems. One open question is whether cognitive mechanisms, such as learning, can maintain the honesty of signals, thus, favoring their evolutionary stability. Here we present evolutionary simulations, where individuals can produce a signal proportional to their quality; and learn along their lifetime the best response to the signal emitted by their peers. In these simulations, learning on the receiver side can mediate the evolution of signals of quality on the sender side. When the cost of the signal is proportional to the quality of the sender populations are only composed of honest signalers. When the cost is not proportional to the quality of the signaler, the population is composed of both honest and dishonest signalers. We argue that learning can be a general cognitive mechanism playing a role in a wide range of communication systems.  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,cache=TRUE,warnings=FALSE)
require(png)
require(here)
source(here("../R_files/posPlots.r"))
source(here("AccFunc.r"))
```

## Introduction 

The outcome of interactions with con-specifics is an important 
determinant of fitness in social animals. Irrespective of whether 
such interactions are cooperative or competitive, the actions of 
interacting individuals partly determine 
their reproductive success. However, the best action for an individual 
might vary depending on its condition, and that of individuals with 
whom it interacts. Thus, acting based on information about interacting 
partners is typically adaptive [@quinones_Negotiation_2016].
Acquiring this information, however, is far from trivial. 
In some cases, interacting partners (signallers) might be 'willing' to 
provide accurate information, but in others it might be in their 
own interest to conceal information [@johnstone_Recognition_1997], 
or to provide erroneous information [@johnstone_Badges_1993]. 
Typically, in a given context, some proportion of individuals will benefit 
from broadcasting accurate information, whereas others will benefit 
from hiding it. Take, for instance, an interaction between two individuals 
where one can help the other. Given some costs and benefits, 
a potential donor will be interested in helping 
related individuals. Therefore, for a relative of the donor, 
broadcasting kinship is advantageous, whereas for an unrelated individual, 
concealing the lack of kinship is better. Similar scenarios may apply to 
many aspects of social life such as finding mates, 
feeding offspring, dominance relationships and aggressive interactions 
[@bradbury_Principles_2011; @moller_Female_1988; @tibbetts_Socially_2004].
In dominance and aggressive interactions, an important piece of
information to guide individual actions is interacting 
partners fighting ability, which is sometimes referred to as 
Resource Holding Potential (RHP) 
[@parker_Assessment_1974], or simply as quality. 
Responsiveness to the quality of the partner is central 
to communication systems such as badges of status, 
where an arbitrary signal conveys quality 
[@johnstone_Badges_1993; @rohwer_Social_1975a]. 
Here, arbitrary refers to a signal which is not 
ontogenetically correlated with quality. 
Badges of status can be evolutionarily stable whenever the 
signal imposes a fitness cost which decreases with the 
quality of an individual  [@botero_Evolution_2010; @johnstone_Badges_1993], 
such that it's no longer in the interest of low quality individuals 
to fake quality by producing a large badge. The cost of honest signals 
is a more general principle of communications systems, 
usually referred to as the handicap principle 
[@grafen_Biological_1990; @zahavi_Mate_1975]. 
<!-- should this last part of the paragraph stay ?-->
<!-- An alternative system of communication is one where individuals  -->
<!-- gather and store information about each individual they interact with.  -->
<!-- This information should reliably allow each individual to react adaptively  -->
<!-- to the quality difference with their opponent. This type of  -->
<!-- assessment strategy is often referred to as individual  -->
<!-- recognition [@whitfield_Plumage_1986]. Evidently, individual recognition  -->
<!-- is restricted to  certain interaction structures, because individuals  -->
<!-- are limited by cognitive abilities in how many individuals  -->
<!-- they can recognize and track. Thus, individual recognition and  -->
<!-- badges of status have been assumed to be alternative communication  -->
<!-- systems fulfilling the same function, reducing the costs of  -->
<!-- aggression in contests over resources.  -->

<!-- Furthermore, badges of status and individual recognition are expected  -->
<!-- to evolve under different interaction structures, badges of status under  -->
<!-- global interactions and individual recognition -->
<!-- under local interactions [@dale_Signaling_2001; @sheehan_There_2016].   -->
<!-- There is no clear cut reason, however, why both systems could not  -->
<!-- work together, in fact there is some evidence -->
<!-- that they can [@chaine_Manipulating_2018]. -->

A somewhat ignored component of communication systems, in the context 
of aggressive interactions, is the cognitive aspects of the receiver module. 
Theoretical models often assume that communication systems relying on badges of 
status to have reaction norms as their 
mechanistic underpinning [@botero_Evolution_2010], with 
individuals using their own quality and the opponent's badge to determine 
whether to aggressively engage in contest. Reaction norms 
allow individuals to respond to the available information without a big
cognitive burden. That is at least in contrast to other information processing 
mechanisms like individual recognition. With individual recognition 
individuals associate cue of their peers to their quality. These association 
must be learn thoughout individuals lifes, thus it has the usual 
cognitive requirements of an associative learning process. An alternative view 
of badges of status is that individuals learn 
to react to them based on their experiences [@guilford_Receiver_1991], 
which would imply that receivers must learn to associate signals with 
the fighting ability of bearers just as in systems based on individual 
recognition. In cases where badges of status vary quantitatively 
(e.g. in size or intensity), fighting ability may increase monotonically 
with attributes of the badge and would be reinforced by every interaction, 
so in principle learning would be faster than in systems involving individual 
recognition in which the association between signals and their meaning would 
vary depending on the interacting partners. In any case, learning could be 
a central cognitive mechanism in both types of communication systems, 
but the role of learning in these contexts has not been thoroughly 
explored in the empirical nor theoretical literature. 

Associative learning is a key cognitive mechanism that allows 
individuals to associate rewards with environmental stimuli 
and thus behave adaptively [@staddon_Adaptive_2016]. Associative 
learning exists in all major vertebrate taxa, and in many invertebrates
as well [@heyes_Simple_2012;@macphail_Brain_1982; @staddon_Adaptive_2016; 
@behrens_Associative_2008].
Theory has shown that natural selection favours these associations 
in complex environments where conditions are hard to predict 
[@dridi_Environmental_2016]. Besides its wide taxonomic and
ecological relevance, associative learning is a flexible cognitive 
mechanism whose underpinnings show interspecific variation 
[@enquist_Power_2016; @quinones_Reinforcement_2019], so presumably, it has been modified 
by natural selection. Despite all this, associative learning is 
not often included in the narrative of evolutionary explanations of 
behavioural patterns [@fawcett_Exposing_2013; @kamil_Optimal_1983; 
@mcauliffe_Psychology_2015]. Computational models of evolution 
can play an important role to overcome the lack of integration 
between learning and evolution. Reinforcement learning theory 
encompasses a series of computational methods inspired 
on the psychological and neurological mechanisms of associative 
learning [@sutton_Reinforcement_2018]. These set of algorithms allow the 
implementation of biologically realistic problems, 
capturing the essence of learning processes 
[@frankenhuis_Enriching_2018; @quinones_Reinforcement_2019]. 
Furthermore, these algorithms can be embedded in evolutionary simulations to 
generate theoretical predictions of the effect that learning can 
have in behavioural evolution [@leimar_Learning_2019].

In here we present an evolutionary model where individuals use 
associative learning to develop a tendency to behave aggressively 
(or  peacefully), in the context of competition over resources, 
depending on the quantitative morphological trait they 
perceive in their opponent (badge). Over evolutionary time 
individuals evolve the size of their badge, and whether it depends 
on their quality. Under this simple
set up, individuals can use the badge as a signal of quality. 
We use the model to assess under what conditions 
of interaction structure we expect different communication signals to evolve. 

## The model

We model the evolution of signals of quality in the context of agonistic 
interactions. For simplicity we consider a population of haploid 
individuals with non-overlapping generations. Individuals are born 
every generation with a level of quality ($Q_i$, where $i$ is subscript 
of the population vector of size $N$) given by a number between zero and one, 
which is drawn from a truncated normal distribution 
$N( 0.5,\sigma)$. An individual with quality $0$ has the lowest RHP, 
while an individual with quality $1$ has the highest. As part of 
development processes individuals produce a morphological signal (badge), 
the size of such depends on their level 
of quality according to a reaction norm given by 
$B_i=1/(1+e^{\alpha_i-\beta_i Q_i})$; where $\alpha_i$ and 
$\beta_i$ are individual specific traits that determine the shape of 
the reaction norm (Fig. \ref{fig:model-struc} A). Variation in $\alpha_i$ 
and $\beta_i$ means individuals can have either uninformative (flat) or 
informative (logistic) norms. The size of the signal is constrained 
between zero and one. We assume different values of these traits 
are given by different alleles, and are inherited from mother to offspring. 
Unless, with a small probability ($\mu$), mutation changes the allelic value 
of the offspring by an amount taken from a normal distribution $N(0,\sigma_\mu)$.
After birth individuals go through a round of 
viability selection. Individual specific survival probability is 
given by $s_i=1/(1+e^{-k_1-k_2(Q_i-B_i)})$. Where $k_1$ and $k_2$ are parameter 
values determining the shape of the survival function. 
Importantly, if $k_2=0$ survival probability is independent of quality, 
while if $k_2>0$ lower quality individuals pay a higher price for similar-sized 
badge, fulfilling the core assumption of the handicap principle 
[@botero_Evolution_2010; @grafen_Biological_1990; @johnstone_Badges_1993].

Individuals who survive engage in a series of pairwise interactions 
where they compete for resources. Individuals in real populations typically 
do not interact at random nor with all individuals in the population 
[@kurvers_Evolutionary_2014]. In order to allow for non-random interactions, 
we use the population vector to define an interaction neighbourhood 
for the focal individual, the size of the neighbourhood ($g$)
is the number of positions around the focal's from which the partner is drawn. 
So, if $g=N$ interactions are global; as $g$ gets smaller 
interactions are more local. In an interaction each individual must 
decide whether to escalate a fight or not. 
Following the classic *hawk-dove* game [@maynard-smith_Evolution_1982], 
if the focal individual fights and its partner does not, the focal gets 
as pay-off the resource of value $V$ while its partner gets nothing; 
if both individuals restrain  from fighting they split up the resource 
in half; if both decide to fight they split up the cost of fighting and 
the winner takes over the resource. We further assume that the 
probability of wining a fight for the focal individual depends on the 
difference in quality between her and her interacting partner; specifically 
it is given by $p_{ij}=1/(1+e^{-k_3(Q_i-Q_j)})$, where $k_3$ is a 
parameter defining how strong the quality difference determines the 
winning probability; and $i$ and $j$ denote the the position 
in the population vector of the focal and its interacting 
partner respectively.

The decision of whether to escalate a fight against an interacting 
partner can be dependent on the size of the partner's badge, 
the actual dependency is determined by the focal´s experiences 
through a learning process. We implement learning using 
the actor-critic approach from reinforcement learning (RL) theory [@sutton_Reinforcement_2018;@quinones_Reinforcement_2019; @leimar_Learning_2019].
Individuals estimate the reward (pay-off) expected from  interacting 
with partners of different badge sizes (the critic in RL terminology). 
After every interaction they update the estimate of reward proportionally 
to the difference between their current estimate and the observed 
reward (prediction error $\delta$) and to the speed of learning 
($\mathrm{A}$). Furthermore, they express different probabilities of 
retreating (attacking) depending on the badge size of their opponent 
(the actor in RL). They update the probability of 
retreating (attacking)  up or down depending on whether retreating 
(attacking) leads to an increase in the reward estimation. 
So, if a focal individual decides to escalate a fight against 
an individual with small badge, and that leads to an increase in 
the reward estimation, the focal individual will increase the probability 
of escalating fights with individuals of small badges in the future. 
Given that badge size is a real number between 0 and 1, there are infinitely 
many badge sizes. Thus, the reward estimation, as well as the probability 
of retreating (escalating), must be generalized across different values. 
To implement generalization we use the linear function approximation 
method based on radial basis functions [@sutton_Reinforcement_2018]. 
Specifically, we pick $c$ feature centres, which are evenly spaced values 
along the badge size interval ([0,1]). Each one of these features centres 
is associated with a weight for reward estimation and tendency to play 
retreat in a given interaction. The reward estimation and the tendency 
to retreat are calculated (in every interaction) as the sum of the weights 
associated with each feature, weighted by the response 
triggered by the feature (Fig. \ref{fig:model-struc} B, dots 
represent the feature weights) The response of each feature centre 
diminishes as a Gaussian function  with the distance between the 
feature centre and the badge size of 
the partner (Fig. \ref{fig:model-struc} B, grey line). Formally the 
reward estimation $\hat{R}$ when the focal individual ($i$) faces 
individual $j$ is given by,
\
\begin{equation}
\hat{R}_{ij} =  \sum_{z=1}^{c} x_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where $x_z$ is the weight of feature centre $z$ on the reward estimation; 
and $\theta$ is the width of the generalization function. Similarly, 
the tendency to retreat is given by the sum of feature weights associated 
with the actor, and the probability is obtained by applying a logistic 
transformation (Fig. \ref{fig:model-struc} B, black line). 
So formally, the log-odds to retreat when facing individual $j$ is 

\begin{equation}
logit(q_{ij}) =  \sum_{z=1}^{c} y_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where $y_z$ is the weight of feature centre $z$ on the tendency to retreat.

After all their interactions, individuals in the population 
reproduce proportionally to their fitness $w_i$, which is a sum of the 
baseline fitness ($w_0$) and all the pay-offs obtained throughout their 
life. Thus, the combination of natural selection and genetic drift 
changes the distribution of values in $\alpha$ and $\beta$ that segregates 
in the population, effectively changing  the badges expressed and the 
communication system. 

```{r model-struc,echo=FALSE,warning=FALSE,fig.width=8,fig.align='center',fig.height=7,fig.show='hold',fig.cap= "Model of communication in the context of aggressive interactions. In A the sender code: a reaction norm determines how the badge size is determined by the quality of the individual. Red shows an informative reaction norm, while the blue shows a uninformative reaction norm. B the receiver code: individuals have a behavioral reaction norm that determines their probability of retreating (black line). The reaction norm arises from generalizing the information from the feature weights (black dots). Generalization is represented by the grey line and its axis, which shows the response triggered by the fourth feature diminishes as the value evaluated is further from the feature center. The learning process moves the feature weights (black dots) up or down depending on whether that leads to an increase in the estimated reward. In C and D, effect of learning on the receiver strategy. Receivers in C face signallers with uninformative reaction norms (blue line in A), while in D they face signallers with informative reaction norms (red line in A). Colour scale in C (applies also for D) indicates the quality of the individual"}

# Top panels -  A the sender code and  B learning for the receiver code

library(dplyr)
honestBig<-readPNG(here("Images/cart_honestBig.png"))
honestSmall<-readPNG(here("Images/cart_honestSmall.png"))
dishonest<-readPNG(here("Images/cart_dishonest.png"))
par(plt=posPlot(numplotx = 2,idplotx = 1,numploty = 1,idploty = 1,upboundx = 91),
    las=1,mfrow=c(2,1),mgp=c(2,0.7,0))
  rangPars<-data.frame(honest=c(4,2),dishonest=c(0,0))
 rangQual<-seq(0,1,by = 0.01)
 linesSender<-sapply(rangPars,function(x)logist(rangQual,alpha = x[2],beta = x[1]))
 plot(0,0,type="l",xlab="",xlim=c(0,1),
      ylab ="",cex.lab=1,lwd=2,col="grey",cex.axis=1,ylim=c(0,1.1),xaxt="n")
 axis(1)
 text(x = 0.08,y=1.05,"A",cex = 1.5)
 mtext("Quality",1,line = 2,cex = 1,las = 0)
 matlines(x=rangQual,y=linesSender,lwd=4,col=colRuns,lty=1)
 mtext("Badge size",2,line = 2.3,cex = 1,las = 0)
 # include bird cartoons
 rasterImage(honestBig,0.75,0.87,0.95,1.07)
 rasterImage(honestSmall,0.05,0.01,0.25,0.21)
 rasterImage(dishonest,0.75,0.5,0.95,0.7)
 rasterImage(dishonest,0.05,0.5,0.25,0.7)
 # legend("left",legend=rangeBeta,col=colRuns,lwd=3,title = expression(beta),cex=2,bty="n")
 
par(plt=posPlot(numplotx = 2,idplotx = 2,numploty = 1,idploty = 1,
                 upboundx = 91),las=1,new=TRUE)
  nCenters<-6
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
sigSq<-0.01
weights<-rep(0,nCenters)
weights<-c(-5,-2,-1,1,2,5)#runif(nCenters,min=-1,max=1)
rangx<-seq(0,1,length=1000)

plot(logist(totRBF(rangx,centers,sigSq,weights),alpha = 0,beta=1)~rangx,type='l',col=1,
     xlab="",ylab="",ylim=c(0,1.1),lwd=3,xaxt="s",cex.lab=2,cex.axis=1,yaxt='n')
axis(4,cex=1)
text(x = 0.08,y=1.05,"B",cex = 1.5)
# test<-sapply(1:6, function(x){RBF()})
points(y=logist(weights,alpha = 0,beta=1),x=centers,cex=1.5,pch=20)
respon<-logist(weights[4]*RBF(rangx,centers[4],0.01))
lines(x=rangx,y=respon,col="grey",lwd=2)
axis(side = 4,at = seq(range(respon)[1],range(respon)[2],length.out = 4),
     labels = round(seq(0,1,length.out = 4),2),line = -4,col="grey")


mtext("prob. retreat",4,line = 2.3,cex = 1,las = 0)
mtext("Badge size",1,line = 2,cex = 1,las = 0)

## Lower panels - the effect of learning

scenario1<-"alphaActDishonest"
scenario2<-"alphaActHonest"

SimsDir<-here("Simulations",paste0(scenario1,"_"))
listTest<-list.files(SimsDir,full.names = TRUE)
indList<-grep("ind",listTest,value=TRUE)
# parameter values from project folder
paramName<-list.files(here("Simulations",paste0(scenario1,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE)
param<-fromJSON(here("Simulations",paste0(scenario1,"_"),paramName[2]))
# fromJSON(paramName[1])
fileId<-1 # choose the replicate

indLearn<-fread(indList[fileId]) # load file
Valpar<-gsub("[[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
nampar<-gsub("[^[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
 
gener<-0#tail(indLearn[,unique(time)],2)[2]
nCenters<-param$nCenters
sigSquar<-param$sigSq
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
# nCenters<-5
# interv<-1/nCenters
# centers<-interv*0.5+interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=100)
cexAxis<-2
colorbreaksQual<-seq(0,1,length=100)
tempPop<-indLearn[time==gener]
behavTime<- tempPop[,tail(unique(nInteract),2)[1],by=indId][,min(V1)]
  
rangx<-seq(0,1,by=0.01)
  dataIndsAct<-sapply(as.list(tempPop[nInteract==behavTime,indId]),
                      function(x){x=
                        logist(totRBF(rangx,
                                      centers,sigSquar,
                                      as.double(
                                        tempPop[(nInteract==behavTime
                                                 &indId==x)
                                                # &(Quality>0.8&Quality<1)
                                                ,.SD,
                                                .SDcol=grep("WeightAct",
                                                            names(tempPop),
                                                            value = TRUE)
                                                ])),alpha = 0,beta = 1)})
  # par(plt=posPlot(numploty = 4,numplotx = 4,idploty = 2,idplotx = 1))
  par(plt=posPlot(numploty = 1,numplotx = 2,idploty = 1,idplotx = 1,upboundx = 91))
matplot(x=rangx,y=dataIndsAct,type='l',xlab="",ylab="prob. retreat",
          xaxt="s",yaxt="s",lty = 1,
          col=paletteMeans(100)[
            findInterval(tempPop[nInteract==1700,Quality],colorbreaksQual)],
          lwd=3,ylim=c(0,1.1))
text(x = 0.08,y=1.05,"C",cex = 1.5)
par(new=FALSE)
color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "quality",
               numplotx = 20,numploty = 10,idplotx =9,idploty = 2)

SimsDir<-SimsDir<-here("Simulations",paste0(scenario2,"_"))
listTest<-list.files(SimsDir,full.names = TRUE)
indList<-grep("ind",listTest,value=TRUE)
# parameter values from project folder
paramName<-list.files(here("Simulations",paste0(scenario1,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE)
param<-fromJSON(here("Simulations",paste0(scenario1,"_"),paramName[2]))
# fromJSON(paramName[1])
fileId<-1 # choose the replicate

indLearn<-fread(indList[fileId]) # load file
Valpar<-gsub("[[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
nampar<-gsub("[^[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
 
gener<-0#tail(indLearn[,unique(time)],2)[2]
nCenters<-param$nCenters
sigSquar<-param$sigSq
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
# nCenters<-5
# interv<-1/nCenters
# centers<-interv*0.5+interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=100)
cexAxis<-2
colorbreaksQual<-seq(0,1,length=100)
tempPop<-indLearn[time==gener]
behavTime<- tempPop[,tail(unique(nInteract),2)[1],by=indId][,min(V1)]
  
rangx<-seq(0,1,by=0.01)
  dataIndsAct<-sapply(as.list(tempPop[nInteract==behavTime,indId]),
                      function(x){x=
                        logist(totRBF(rangx,
                                      centers,sigSquar,
                                      as.double(
                                        tempPop[(nInteract==behavTime
                                                 &indId==x)
                                                # &(Quality>0.8&Quality<1)
                                                ,.SD,
                                                .SDcol=grep("WeightAct",
                                                            names(tempPop),
                                                            value = TRUE)
                                                ])),alpha = 0,beta = 1)})
  # par(plt=posPlot(numploty = 4,numplotx = 4,idploty = 2,idplotx = 1))
  par(plt=posPlot(numploty = 1,numplotx = 2,idploty = 1,idplotx = 2,upboundx = 91)
      ,new=TRUE,xpd=TRUE)
matplot(x=rangx,y=dataIndsAct,type='l',xlab="",ylab="",
          xaxt="s",yaxt="n",lty = 1,
          col=paletteMeans(100)[
            findInterval(tempPop[nInteract==1700,Quality],colorbreaksQual)],
          lwd=3,ylim=c(0,1.1))
text(x = 0.08,y=1.05,"D",cex = 1.5)
text(x = -0.1,y = -0.25,labels = "Badge size")



```


## Results

We first present the outcome of simulations where we prevent the 
evolution of the sender code (by setting the mutation rate to $0$), 
and assume that all individuals in the population display either 
an uninformative or informative badge size with respect to their quality. 
These simulations show what type of receiver strategy develops through 
a learning process. Figure \ref{fig:model-struc} (C and D) shows 
the receiver strategy developed through learning; panel C is for 
receivers that faced uninformative signals, and D informative ones. 
When learners face uninformative signals, they change their 
probability of retreating depending on their own quality. Individuals 
with higher quality (red tones), after the learning process, have a 
high probability of attacking; while individuals of lower quality 
(blue tones) mostly retreat from confrontations. Thus, learning splits
the population of receivers into the two classic pure strategies of 
hawks and doves. Given that we have a assumed a monomorphic population 
with unresponsive reaction norms on the signalling side, the changes 
triggered by learning only affect a small range of badge sizes. 
In contrast, when receivers face informative reaction norms 
on the side of the signaller (panel B in fig. \ref{fig:model-struc}), 
receivers use the badge size of their interacting partners to determine 
whether to retreat or attack. The relationship is given by a 
threshold-like reaction norm, where the change from retreating to 
attacking depends on the quality of the receiver. As expected, 
the higher the quality of the receiver the larger the badge size 
that triggers a retreat.

```{r handicap,echo=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap="Evolution of the badge size as a handicap mediated by learning. Middle panels show the evolutionary dynamics of the sender code. On the left, changes in the distribution of values for the intercept of the reaction norm ($\\alpha$); on the right changes in the distribution of the slope ($\\beta$). Dashed lines in both panels show the mean of the distribution. Grey lines show the generation time corresponding to the panels above and below showing the sender and receiver code. In the panels above, the reaction norms correspond to individuals in the population after the interaction round. Color scale indicates quality just as in Fig. \\ref{fig:model-struc}. In the panels below, colors represent a cluster clasification performed by the k-means algorithm" ,warning=FALSE}

scenario<-"betCostEvol4"#"alphaAct"#"nIntGroupNormQual"#

# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)
paramName<-list.files(here("Simulations",paste0(scenario,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*5_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))
  # fromJSON(paramName[1])
val<-2
fileId<-val
evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)
evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
runChoi<-1
# Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 5,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
cexAxis<-1

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])*c(2,1.4),xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("top",legend = c(expression(alpha[s])),col=colGenesLin[1],lwd=2,
       bty = "n",cex=cexAxis)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,3),4),nrow = 2),lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 2,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    xaxt="s",las=1,new=TRUE)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])*c(2,1.4),xlim=c(0,22000))
legend("top",legend = c(expression(beta[s])),cex=cexAxis,
       col=colGenesLin[2],lwd=2,bty = "n")
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,6),4),nrow = 2),lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin[2],lty = 2,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 3,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 90),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, length(tempPop$indId), replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1),col = paletteMeans(100)[
            findInterval(tempPop[,Quality],colorbreaksQual)],lty = 1,
          cex.axis=cexAxis)
  axis(side = 3,cex.axis=cexAxis,padj = 0.1)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.7,side = 3 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8,lowboundy = 10,
                  upboundy = 90), las=1,new=TRUE)

  # meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
  # reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
  #   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
  #          beta = meansClustmp[orderClus==x,betaMean])})
  # matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
  #          type='l',cex.axis=cexAxis,
  #          xlab="",ylab="",ylim=c(0,1),lty=1,
  #          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1),lty=1,col=1,
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.5,side = 1,las=0)
  
  }

rm(list=grep("temp",ls(),value = T))
```


When we allow the evolution of the sender code ($\alpha$ and $\beta$ 
change subject to natural selection and genetic drift), and the badge 
size work as a handicap (i.e. the cost of the badge is inversely 
proportional to the quality of the individual), the sender code evolves 
to produce an honest signal Fig \ref{fig:handicap}). The evolution of 
the sender code does not happen immediately after the start of the 
evolutionary process. Instead, the evolutionary dynamics of the reaction 
norm parameters (Fig. \ref{fig:handicap}, X) show a set of 
steps. First, the intercept of the reaction norm ($\alpha$) evolves to 
higher values, reducing the average badge size in the populations. 
That makes sense because reaction norms segregating in the population are 
flat at this point, and consequently the receiver code does not respond to 
the badge size, bigger badges are costlier and do not trigger lower attack 
probabilities. During those first generations, the slope of the sender 
reaction norm ($\beta$)  remains around the initial value of zero. 
At around generation 4000, the slope evolves toward positive values. 
Which implies that larger badges correlate with higher quality. Receivers 
learn to react to such correlation, reducing the probability of attack towards 
individual of larger badges. Hence, natural selection favours larger 
values of $\beta$, eventually leading to an evolutionary equilibrium where 
badge size is an honest signal of quality mediated by the learned responses 
of receivers. The evolutionary trajectory portrayed in figure 
\ref{fig:handicap} is not the only possible trajectory. If the slope of the 
sender reaction norms ($\beta$) evolves toward negative values before badges 
become handicaps, receivers never learn to react to the size of badges. 
Thus, badges are only costly and do not provide information about the 
quality of an individual(Fig. \ref{fig:noBadge} . Eventually, the badge disappears from the population. 

When production of the badge is not costly (i.e. does not work as a handicap), 
the parameters of the sender code go through evolutionary branching event, 
triggering the evolution of different types of signals in the population. 
Fig. \ref{fig:branching}A and B shows changes in the distribution of the 
sender reaction norm ($\alpha$ and $\beta$) along evolutionary time. 
Darker areas show trait values with high frequency in the population. 
Populations start monomorphic with a value of zero on both traits, 
and mutations quickly build up a normal distribution around the starting 
value. Within the first 2000 generations the unimodal distribution splits 
into a bimodal one. In the case of $\alpha$, one of the peaks splits further, 
so at the end of the evolutionary simulation the distribution of $\alpha$ 
value in the population shows three peaks. In the case of $\beta$, the two 
initial peaks coexist for around 4000 generations. Later on one of the 
types goes extinct and the distribution goes back to being unimodal. 
These changes in the distribution of the sender reaction norm parameters 
imply that during the second half of the simulation individuals can be 
classified into three distinct types. Two types express a flat reaction 
norm, meaning that their badge size is not informative of their quality, 
while the third type shows intermediate badge sizes which are determined 
by the quality of the individual (Fig. \ref{fig:branching}). Furthermore, 
the receiver reaction norms develop through learning, particularly those 
of individuals of intermediate quality, respond to the signal of their 
sender type by increasing the probability of retreating from a fight with 
individuals with larger badges (Fig. \ref{fig:branching}). 

```{r branching,echo=FALSE,out.width='90%',fig.align='center',fig.show='hold',fig.cap= "The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \\ref{fig:handicap}. The middle panels show changes in the distribution of values fir $\\alpha$ and $\\beta$ along evolutionary time. Panels above and bellow correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels."}

# knitr::include_graphics(here("Simulations","nIntGroupNormQual_","evolDyn7_nIntGroup2000.png"))
val<-2
fileId<-val

paramName<-list.files(here("Simulations",paste0(scenario,"_")))
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*s0_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))


evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
runChoi<-2
### Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
cexAxis<-1

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])*c(2,1.4),xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("top",legend = c(expression(alpha[s])),col=colGenesLin[1],lwd=2,
       bty = "n",cex=cexAxis)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,3),4),nrow = 2),lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 2,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    xaxt="s",las=1,new=TRUE)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])*c(2,1.4),xlim=c(0,22000))
legend("top",legend = c(expression(beta[s])),cex=cexAxis,
       col=colGenesLin[2],lwd=2,bty = "n")
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,6),4),nrow = 2),lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin[2],lty = 2,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
sizeSamp<-30
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 3,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 90),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, size = sizeSamp, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],colorbreaksQual)],
          lty = 1,cex.axis=cexAxis)
  axis(side = 3,cex.axis=cexAxis,padj = 0.1)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.5,side = 3 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8,lowboundy = 10,
                  upboundy = 90), las=1,new=TRUE)
  # dataIndReact<-sapply(as.list(tempPop[,indId]),
  #                      function(x){x=
  #                        sapply(rangx, 
  #                               function(y)
  #                                 do.call(logist,
  #                                         as.list(
  #                                           c(y,as.double(
  #                                             tempPop[indId==x,.SD,
  #                                                     .SDcol=c("alpha","beta")])))))})
  # meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
  # reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
  #   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
  #          beta = meansClustmp[orderClus==x,betaMean])})
  # matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
  #          type='l',cex.axis=cexAxis,
  #          xlab="",ylab="",ylim=c(0,1),lty=1,
  #          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
   reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1),lty=1,col=1,
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.5,side = 1,las=0)
  
  }

# rm(list=grep("temp",ls(),value = T))

```

```{r startCond, echo=FALSE, out.width='90%',fig.align='center',fig.show='hold',fig.cap="WHy would you want to be recognized"}

knitr::include_graphics("../Simulations/initAct_/indVarScatter_.png")

```


# Supplementary material

```{=tex}
\beginsupplement
```

```{r noBadge,echo=FALSE,warning=FALSE,out.width='90%',fig.align='center',fig.show='hold',fig.cap= "The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \\ref{fig:handicap}. The middle panels show changes in the distribution of values fir $\\alpha$ and $\\beta$ along evolutionary time. Panels above and bellow correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels."}

# knitr::include_graphics(here("Simulations","betCostEvol1_","evolDyn5_betCost5.png"))
val<-2
fileId<-val

paramName<-list.files(here("Simulations",paste0(scenario,"_")))
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*s0_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))


evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
runChoi<-3
### Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
cexAxis<-1

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])*c(2,1.4),xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("top",legend = c(expression(alpha[s])),col=colGenesLin[1],lwd=2,
       bty = "n",cex=cexAxis)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,3),4),nrow = 2),lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 2,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 90),
    xaxt="s",las=1,new=TRUE)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])*c(2,1.4),xlim=c(0,22000))
legend("top",legend = c(expression(beta[s])),cex=cexAxis,
       col=colGenesLin[2],lwd=2,bty = "n")
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(c(-2,6),4),nrow = 2),lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin[2],lty = 2,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
sizeSamp<-30
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 3,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 90),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, size = sizeSamp, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],colorbreaksQual)],
          lty = 1,cex.axis=cexAxis)
  axis(side = 3,cex.axis=cexAxis,padj = 0.1)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.5,side = 3 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8,lowboundy = 10,
                  upboundy = 90), las=1,new=TRUE)
  # dataIndReact<-sapply(as.list(tempPop[,indId]),
  #                      function(x){x=
  #                        sapply(rangx, 
  #                               function(y)
  #                                 do.call(logist,
  #                                         as.list(
  #                                           c(y,as.double(
  #                                             tempPop[indId==x,.SD,
  #                                                     .SDcol=c("alpha","beta")])))))})
  # meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
  # reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
  #   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
  #          beta = meansClustmp[orderClus==x,betaMean])})
  # matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
  #          type='l',cex.axis=cexAxis,
  #          xlab="",ylab="",ylim=c(0,1),lty=1,
  #          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
   reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1),lty=1,col=1,
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.5,side = 1,las=0)
  
  }


```

| Symbol | Description |
|-------:|------------:|
| $Q_i$ | Quality of individual $i$ |
| $N$   | Population size |
| $\sigma$ | Standard deviation of the truncated normal distribution from which quality is drawn |
| $B_i$ | Badge size of individual $i$ |
| $\alpha_i$ | Intercept of the badge size reaction norm for individual $i$ |
| $\beta_i$  | Slope of the badge size reaction norm for individual $i$ |
| $ \mu$     | Mutation rate | 
| $\sigma_\mu$ |Standard deviation of the normal distribution from which quality is mutations are drawn |
| $s_i$ | Survival probability of individual $i$ |
| $k_1$ | Intercept of the survival function |
| $K_2$ | Slope of the survival function |
| $V$   | Value of the contested resource |
| $C$   | Cost of an escalated fight |
| $p_{ij}$ | Probability that individual $i$ wins an escalated fight against individual $j$ |
| $k_3$ | Parameter determining how important is quality defining the probability of wining |
| $\delta$ | Prediction error |
| $A$ | Speed of learning | 
| $c$ | Number of feature centres | 
| $\hat{R}$ | Reward estimate | 
| 



## References