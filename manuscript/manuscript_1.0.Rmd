---
title: |
  The role of learning in status signalling: a modeling approach
author:
- Andrés E. Quiñones:
    email: "andresqp@gmail.com"
    institute: [andes, unine]
    correspondence: true
- Redouan Bshary:
    institute: unine
- C. Daniel Cadena:
    institute: andes
institute:
- andes: Laboratorio de Biología Evolutiva de Vertebrados, Departamento  de Ciencias Biológicas, Universidad de los Andes, Bogotá, Colombia
- unine: Institute of Biology, University of Neuchâtel, Neuchâtel, Switzerland
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex  # default  ##word_document: default  # word_document:default
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
  word_document: default
fig_caption: yes
fontsize: 12pt
header-includes: |
  ```{=latex}
    \usepackage{float}
    \usepackage{unicode-math}
    \usepackage{setspace}\doublespacing
    \usepackage[backref=page]{hyperref}
    \linespread{2}
    \usepackage{lineno}
    \linenumbers
    \floatplacement{figure}{H}
    \newcommand{\beginsupplement}{ \setcounter{table}{0}         \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
  ```
keep_tex: yes
keywords: My keywords
linespread: 2
bibliography: ../SocialMorphCue.bib
spacing: double
style: C:\Users\andre\Zotero\styles\elife.csl
abstract: |
  Adaptive behavioural responses often depend on qualities of the interacting
  partner of an individual. For example, when competing for resources, an individual
  might be better off escalating fights with individuals of lower quality, while
  restraining  from fighting individuals of higher quality. Communication systems
  involving signals of quality allow individuals to reduce uncertainty regarding the
  fighting ability of their partners and make more adaptive behavioral decisions. 
  However, dishonest individuals can destabilize such communications systems. 
  An open question is whether cognitive mechanisms, such as learning, 
  can maintain the honesty of signals, thus favoring their evolutionary 
  stability. We present evolutionary simulations where individuals 
  can produce a signal proportional to their quality and learn along their
  lifetime the best response to the signals emitted by their peers. Our simulations
  replicate previous results where the handicap principle mediates the evolution 
  of signals. In our simulations learning on the receiver side can mediate the evolution
  of signals  of quality on the sender side. When the cost of the signal is proportional
  to the  quality of the sender, all individuals in populations are honest signalers.   
  In contrast to traditional models which predict the absence of signals when the
  cost is not proportional to the quality of the signaler, our model revealed that
  learning facilitates the evolution of a polymorphism in which populations comprise both
  honest and dishonest signalers. We argue that learning may have a role in the evolution
  and dynamics of a wide range of communication systems and more generally in 
  behavioral responses.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,cache=TRUE,warnings=FALSE)
require(png)
require(here)
library(dplyr)
source(here("../R_files/posPlots.r"))
source(here("AccFunc.r"))
library("scales")
```

## Introduction 

The outcome of interactions with conspecific individuals is a critical
determinant of fitness in social animals. Irrespective of whether 
such interactions are cooperative or competitive, the actions of 
interacting individuals influence 
their reproductive success. However, the best action for an individual 
engaging in a social interaction might vary depending on its 
own condition, and that of individuals with 
whom it interacts. Thus, acting based on information about interacting 
partners is typically adaptive [@quinones_Negotiation_2016], yet
acquiring such information is far from trivial. 
In some cases, interacting partners (*i.e.* signallers) might be 'willing' to 
provide accurate information, but in others it might be in their 
own interest to conceal information [@johnstone_Recognition_1997] 
or to provide deceptive information [@johnstone_Badges_1993]. 
Typically, in a given context, some individuals benefit 
from broadcasting accurate information, whereas others benefit 
from concealing it. Take, for instance, an interaction between two individuals 
where one can help the other. Given some costs and benefits, 
a potential donor is likely to be interested in helping 
related individuals. Therefore, for a relative of the donor, 
broadcasting kinship would be advantageous, whereas concealing the lack 
of kinship would be better for an unrelated individual. 
Similar scenarios may apply to a variety of aspects of social 
life such as finding mates, feeding offspring, or engaging in
dominance relationships or aggressive interactions 
[@bradbury_Principles_2011; @moller_Female_1988; @tibbetts_Socially_2004].

In dominance and aggressive interactions, a crucial piece of
information to guide individual actions is the fighting ability of interacting 
partners, which is sometimes referred to as Resource Holding Potential 
[RHP, Parker -@parker_Assessment_1974] or simply as quality. 
Responsiveness to the quality of an opponent is central 
to communication systems in which individuals 
exhibit badges of status, where a signal conveys quality 
[@johnstone_Badges_1993; @rohwer_Social_1975a]. 
Such signals must be arbitrary, meaning the signals could potentially be 
produced by high and low quality individuals alike. 
Generally signals can be evolutionarily stable whenever low-quality individuals 
pay a higher fitness cost for carrying the signal such that it is not 
in the interest of low-quality individuals to fake quality by producing a large badge.
Signal costs inversely proportional to quality may come about as a result of the
production costs of the signal, in which case the signal works like a handicap
[@botero_Evolution_2010; @johnstone_Badges_1993;@grafen_Biological_1990;@zahavi_Mate_1975]. 
Alternatively, the costs can come from 
the aggressive reaction of receivers when the convention of the communication 
is broken [@enquist_Signaling_2010;@tibbetts_Socially_2004]. For example in paper 
wasps, subordinate individuals with experimentally manipulated dominant-like facial
patterns received more aggression from the dominant[@tibbetts_Socially_2004]. 
In this case, costs are socially rather than developmentally mediated and are 
triggered by the violation of the convention, although how the convention
is established in the first place is unclear.

<!-- should this last part of the paragraph stay ?-->
<!-- An alternative system of communication is one where individuals  -->
<!-- gather and store information about each individual they interact with.  -->
<!-- This information should reliably allow each individual to react adaptively  -->
<!-- to the quality difference with their opponent. This type of  -->
<!-- assessment strategy is often referred to as individual  -->
<!-- recognition [@whitfield_Plumage_1986]. Evidently, individual recognition  -->
<!-- is restricted to  certain interaction structures, because individuals  -->
<!-- are limited by cognitive abilities in how many individuals  -->
<!-- they can recognize and track. Thus, individual recognition and  -->
<!-- badges of status have been assumed to be alternative communication  -->
<!-- systems fulfilling the same function, reducing the costs of  -->
<!-- aggression in contests over resources.  -->

<!-- Furthermore, badges of status and individual recognition are expected  -->
<!-- to evolve under different interaction structures, badges of status under  -->
<!-- global interactions and individual recognition -->
<!-- under local interactions [@dale_Signaling_2001; @sheehan_There_2016].   -->
<!-- There is no clear cut reason, however, why both systems could not  -->
<!-- work together, in fact there is some evidence -->
<!-- that they can [@chaine_Manipulating_2018]. -->

A somewhat ignored component of communication systems in the context 
of aggressive interactions are cognitive aspects of the receiver module. 
Theoretical models often assume that communication systems relying on badges of 
status have reaction norms as their mechanistic underpinning 
[@botero_Evolution_2010], with 
individuals using their own quality and the badges of opponents to determine 
whether to aggressively engage in contests. Reaction norms 
allow individuals to respond to the available information without a large
cognitive burden because the computations involved in the reaction 
norm response do not require much memory nor advanced algorithmic processes. 
This contrasts with other information-processing 
mechanisms like individual recognition. Where individuals associate cues of 
their peers to their quality. Because these 
associations must be learnt throughout the life of individuals, 
they have the usual cognitive requirements of associative learning processes. 
An alternative view of badges of status is that individuals learn 
to react to them based on their experiences [@guilford_Receiver_1991], 
which would imply that receivers must learn to associate signals with 
the fighting ability of bearers just as in systems based on individual 
recognition. In cases where badges of status vary quantitatively 
(*e.g.* in size or intensity), fighting ability may increase monotonically 
with attributes of the badge and would be reinforced by every interaction.
Therefore, in principle, learning in such scenarios would be faster 
than in systems involving individual 
recognition in which the association between signals and their meaning 
varies depending on the interacting partners. In any case, learning may be 
a central cognitive mechanism in both types of communication systems, 
but the role of learning in these contexts has not been thoroughly 
explored in the empirical nor theoretical literature. 


Associative learning is a taxonomically widespread cognitive 
mechanism that allows individuals to associate rewards with 
environmental stimuli and thus behave adaptively 
[@heyes_Simple_2012;@macphail_Brain_1982; @staddon_Adaptive_2016; 
@behrens_Associative_2008]. Theory has shown that natural selection 
favours such associations in complex environments where conditions 
are difficult to predict [@dridi_Environmental_2016]. 
Associative learning is a flexible cognitive 
mechanism whose underpinnings show interspecific variation 
[@enquist_Power_2016; @quinones_Reinforcement_2020;@pretot_Factors_2016;
@pretot_Comparative_2021]. For example, species can have different learning rates
[@hoedjes_Natural_2010], some may vary these rates during learning 
trials which allows them to have more 
flexible learning [@leimar_Flexibility_2023], and some may include future rewards in their associations [@raby_Planning_2007;@osvath_Spontaneous_2009].
Associative learning is not a single mechanism, but rather a set of 
cognitive structures and processes that can vary in their scope and complexity. Presumably, these structures and processes have been modified 
by natural selection and could provide novel explanations for behavioural 
variation. Nonetheless, associative learning is 
not often included in the narrative of evolutionary explanations of 
behavioural patterns [@fawcett_Exposing_2013; @kamil_Optimal_1983; 
@mcauliffe_Psychology_2015].

Computational models of evolution can overcome the lack of integration 
between learning and evolution. Reinforcement 
learning theory encompasses a series of computational methods inspired 
on the psychological and neurological mechanisms of associative 
learning [@sutton_Reinforcement_2018]. This set of algorithms allows the 
implementation of biologically realistic problems, 
capturing the essence of learning processes 
[@frankenhuis_Enriching_2018; @quinones_Reinforcement_2020]. 
Furthermore, these algorithms can be embedded in evolutionary simulations to 
generate theoretical predictions of the effect of learning on behavioural evolution [@leimar_Learning_2019;@leimar_Effects_2022].

Here we present an evolutionary model where individuals use 
associative learning to develop a tendency to behave either aggressively 
or  peacefully in the context of competition over resources, 
depending on a quantitative morphological trait 
(*i.e.* a badge indicating quality) they perceive in their opponents. 
Over evolutionary time, the size of 
the badge evolves as does its dependency on the individual's quality. 
Under this simple set up, individuals can use the badge as a signal of quality. 
We use the model to assess under what conditions 
we expect communication signals to evolve as handicaps or conventions. 

## The model

We model the evolution of signals indicating individual quality 
in the context of agonistic interactions. 
For simplicity we consider a population of haploid 
individuals with non-overlapping generations. Individuals are born 
every generation with a level of quality ($Q_i$, where $i$ is subscript 
of the population vector of size $N$) given by a number between zero and one, 
which is drawn from a truncated normal distribution 
$N( 0.5,\sigma)$. An individual with quality $0$ has the lowest RHP, 
while an individual with quality $1$ has the highest. As they 
develop, individuals produce a phenotypic signal (badge), 
the size of which depends on their quality according to a reaction norm given by 
$B_i=1/(1+e^{\alpha_i-\beta_i Q_i})$; where $\alpha_i$ and 
$\beta_i$ are individual specific traits that determine the shape of 
the reaction norm (Fig. \ref{fig:model-struc} A). Variation in $\alpha_i$ 
and $\beta_i$ means individuals can have either uninformative (flat) or 
informative (logistic) norms. Informative reaction norms represent a 
developmental program where the phenotype of the individual is determined by
the environmental conditions under which it grows, thus causing a 
correlation between quality and signal. The size of the signal is constrained 
to take values between zero and one. We assume different values of these traits 
are given by different alleles and are inherited from mother to offspring 
unless, with a small probability ($\mu$), mutation changes the allelic value 
of the offspring by an amount drawn from a normal distribution $TN(0,\sigma_\mu)$.  

After birth, individuals go through a round of 
viability selection. The survival probability of an individual is 
given by $s_i=1/(1+e^{-k_1-k_2(Q_i-B_i)})$, where $k_1$ and $k_2$ are parameter 
values determining the shape of the survival function. 
Importantly, if $k_2=0$ survival probability is independent of quality, 
while if $k_2>0$ lower-quality individuals pay a higher price for similar-sized 
badges, fulfilling the core assumption of the handicap principle 
[@botero_Evolution_2010; @grafen_Biological_1990; @johnstone_Badges_1993].

Individuals who survive engage in a series of pairwise interactions 
where they compete for resources. 
<!-- Individuals in real populations typically  -->
<!-- do not interact at random nor with all individuals in the population  -->
<!-- [@kurvers_Evolutionary_2014]. In order to allow for non-random interactions,  -->
<!-- we use the population vector to define an interaction neighbourhood  -->
<!-- for the focal individual, the size of the neighbourhood ($g$) -->
<!-- is the number of positions around the focal's from which the partner is drawn.  -->
<!-- So, if $g=N$ interactions are global; as $g$ gets smaller  -->
<!-- interactions are more local.  -->
In each interaction individuals must 
decide whether to escalate a fight or not. 
Following the classic *hawk-dove* game [@maynard-smith_Evolution_1982], 
if the focal individual fights and its partner does not, then the focal gets 
as pay-off the resource of value $V$ while its partner gets nothing; 
if both individuals restrain  from fighting, then they split up the resource 
in half. If both individuals decide to fight, then the winner takes over 
the resource and the cost of fighting is split between the two players. 
We further assume that the probability of wining a fight for the focal 
individual depends on the difference in quality between it and its 
interacting partner; specifically 
it is given by $p_{ij}=1/(1+e^{-k_3(Q_i-Q_j)})$, where $k_3$ is a 
parameter defining how strong the quality difference determines the 
winning probability, and $i$ and $j$ denote the the position 
in the population vector of the focal and its interacting 
partner, respectively.

The decision of whether to escalate a fight against an interacting 
partner can depend on the size of the partner's badge, 
with the dependency being determined by the focal's experiences acquired
through a learning process. We implement learning using 
the actor-critic approach from reinforcement learning (RL) theory [@sutton_Reinforcement_2018;@quinones_Reinforcement_2020; @leimar_Learning_2019;
@leimar_Evolution_2021].
Individuals estimate the reward (pay-off) expected from  interacting 
with partners of different badge sizes (the critic in RL terminology). 
After each interaction they update the estimate of reward proportionally 
to the difference between their current estimate and the observed 
reward (prediction error $\delta$) and to the speed of learning 
($\mathrm{A}$). Furthermore, individuals express different probabilities of 
retreating/attacking depending on the badge size of their opponent 
(the actor in RL). They update the probability of 
retreating/attacking depending on whether retreating/attacking
leads to an increase in the reward estimation. 
Thus, if a focal individual decides to escalate a fight against 
an individual with a small badge and this leads to an increase in 
the reward estimation, then the focal individual will increase the probability 
of escalating fights with individuals of small badges in the future. 
Given that badge size is a real number between 0 and 1, there are infinitely 
many badge sizes. Thus, the reward estimation, as well as the probability 
of retreating/attacking, must be generalized across different values. 
To implement generalization we use the linear function approximation 
method based on radial basis functions [@sutton_Reinforcement_2018].
Specifically, we pick $c$ feature centres, which are evenly spaced values 
along the badge size interval ([0,1]) where the updates are focused. 
For the sake of simplicity we keep the location of these feature centres 
constant and  the same for all individuals; they are stored in 
vector $\symbf{b}$ . Each of these feature centres 
is associated with a weight for reward estimation and tendency to play 
retreat in a given interaction. The reward estimation and the tendency 
to retreat are calculated (in every interaction) as the sum of the weights 
associated with each feature centre, weighted by the response 
triggered by the feature (Fig. \ref{fig:model-struc} B, dots 
represent the feature weights). The response of each feature centre 
diminishes as a Gaussian function with the distance between the 
feature centre and the badge size of 
the partner (Fig. \ref{fig:model-struc} B, grey line). Formally, the 
reward estimation $\hat{R}$ when the focal individual ($i$) faces 
individual $j$ is given by,
\
\begin{equation}
\hat{R}_{ij} =  \sum_{z=1}^{c} x_z e^{(-\frac{|B_i-b_z|}{2\theta^2})} 
\end{equation}

where $x_z$ is the weight of feature centre $z$ on the reward estimation; 
and $\theta$ is the width of the generalization function. Similarly, 
the tendency to retreat is given by the sum of feature weights associated 
with the actor, and the probability is obtained by applying a logistic 
transformation (Fig. \ref{fig:model-struc} B, black line). 
Thus formally, the log-odds to retreat when facing individual $j$ is 

\begin{equation}
logit(q_{ij}) =  \sum_{z=1}^{c} y_z e^{(-\frac{|B_i-b_z|}{2\theta^2})} 
\end{equation}

where $y_z$ is the weight of feature centre $z$ on the tendency to retreat.

Individuals interact as the focal individual $n$ times and the interaction 
partner is chosen at random. Thus, the expected number of interactions for 
each individual is $n+\frac{n(N+1)}{N}$. 
After the interaction round , individuals in the population 
reproduce with a probability proportional to their total pay-off $w_i$, which is a sum of the 
baseline pay-off ($w_0$) and all the pay-offs obtained throughout their 
life. Thus, the combination of natural selection and genetic drift 
changes the distribution of values in $\alpha$ and $\beta$ that segregates 
in the population, effectively changing  the badges expressed and the 
communication system. 

```{r model-struc,echo=FALSE,warning=FALSE,fig.width=8,fig.align='center',fig.height=7,fig.show='hold',fig.cap= "Model of communication in the context of aggressive interactions. In A, the reaction norm determines the badge size, *e.g* the size of a black band on a bird's chest. Red shows an informative reaction norm, whereas blue shows a uninformative reaction norm. B individuals perceiving the signal have a behavioral reaction norm that determines their probability of retreating (black line). The reaction norm arises from generalizing the information from the feature weights (black dots). Feature weights are values associated with a particular badge size and are updated as individuals interact with each other. The generalization of the information associated with each feature weight is represented by the grey line and its axis, which shows that the response triggered by the fourth feature diminishes as the value evaluated is further from the feature center. The learning process moves the feature weights (black dots) up or down depending on whether this leads to an increase in the estimated reward. Panels C and D show the effect of learning on the receiver strategy. Receivers in C face signallers with uninformative reaction norms (blue line in A). The uninformative reaction norm yields a badge size of 0.5, thus the behavioural reaction norm in C is only updated around that value. D Receivers face signallers with informative reaction norms (red line in A). Accordingly, receivers develop a threashold-like reaction norm where the decision to retreat increases with the badge size of the interacting partner. Colour scale in C and D indicates the quality of the individual. "}

# Top panels -  A the sender code and  B learning for the receiver code
# load(here("manuscript","manuscript_1.0_cache","latex","model-struc_6905f4797a1faa21041511a8dcdf1eb9.RData"))
library(dplyr)
honestBig<-readPNG(here("Images/cart_honestBig.png"))
honestSmall<-readPNG(here("Images/cart_honestSmall.png"))
dishonest<-readPNG(here("Images/cart_dishonest.png"))
par(plt=posPlot(numplotx = 2,idplotx = 1,numploty = 1,idploty = 1,upboundx = 91),
    las=1,mfrow=c(2,1),mgp=c(2,0.7,0))
  rangPars<-data.frame(honest=c(4,2),dishonest=c(0,0))
 rangQual<-seq(0,1,by = 0.01)
 linesSender<-sapply(rangPars,function(x)logist(rangQual,alpha = x[2],beta = x[1]))
 plot(0,0,type="l",xlab="",xlim=c(0,1),
      ylab ="",cex.lab=1,lwd=2,col="grey",cex.axis=1,ylim=c(0,1.1),xaxt="n")
 axis(1)
 text(x = 0.08,y=1.05,"A",cex = 1.5)
 mtext("Quality",1,line = 2,cex = 1,las = 0)
 matlines(x=rangQual,y=linesSender,lwd=4,col=colRuns,lty=1)
 mtext("Badge size",2,line = 2.3,cex = 1,las = 0)
 # include bird cartoons
 rasterImage(honestBig,0.75,0.87,0.95,1.07)
 rasterImage(honestSmall,0.05,0.01,0.25,0.21)
 rasterImage(dishonest,0.75,0.5,0.95,0.7)
 rasterImage(dishonest,0.05,0.5,0.25,0.7)
 # legend("left",legend=rangeBeta,col=colRuns,lwd=3,title = expression(beta),cex=2,bty="n")
 
par(plt=posPlot(numplotx = 2,idplotx = 2,numploty = 1,idploty = 1,
                 upboundx = 91),las=1,new=TRUE)
  nCenters<-6
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
sigSq<-0.01
weights<-rep(0,nCenters)
weights<-c(-5,-2,-1,1,2,5)#runif(nCenters,min=-1,max=1)
rangx<-seq(0,1,length=1000)

plot(logist(totRBF(rangx,centers,sigSq,weights),alpha = 0,beta=1)~rangx,type='l',col=1,
     xlab="",ylab="",ylim=c(0,1.1),lwd=3,xaxt="s",cex.lab=2,cex.axis=1,yaxt='n')
axis(4,cex=1)
text(x = 0.08,y=1.05,"B",cex = 1.5)
# test<-sapply(1:6, function(x){RBF()})
points(y=logist(weights,alpha = 0,beta=1),x=centers,cex=1.5,pch=20)
respon<-logist(weights[4]*RBF(rangx,centers[4],0.01))
lines(x=rangx,y=respon,col="grey",lwd=2)
axis(side = 4,at = seq(range(respon)[1],range(respon)[2],length.out = 4),
     labels = round(seq(0,1,length.out = 4),2),line = -4,col="grey")


mtext("prob. retreat",4,line = 2.3,cex = 1,las = 0)
mtext("Badge size",1,line = 2,cex = 1,las = 0)

## Lower panels - the effect of learning

scenario1<-"alphaActDishonest"
scenario2<-"alphaActHonest"

SimsDir<-here("Simulations",paste0(scenario1,"_"))
listTest<-list.files(SimsDir,full.names = TRUE)
indList<-grep("ind",listTest,value=TRUE)
# parameter values from project folder
paramName<-list.files(here("Simulations",paste0(scenario1,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE)
param<-fromJSON(here("Simulations",paste0(scenario1,"_"),paramName[2]))
# fromJSON(paramName[1])
fileId<-1 # choose the replicate

indLearn<-fread(indList[fileId]) # load file
Valpar<-gsub("[[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
nampar<-gsub("[^[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
 
gener<-0#tail(indLearn[,unique(time)],2)[2]
nCenters<-param$nCenters
sigSquar<-param$sigSq
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
# nCenters<-5
# interv<-1/nCenters
# centers<-interv*0.5+interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=100)
cexAxis<-2
colorbreaksQual<-seq(0,1,length=100)
tempPop<-indLearn[time==gener]
behavTime<- tempPop[,tail(unique(nInteract),2)[1],by=indId][,min(V1)]
  
rangx<-seq(0,1,by=0.01)
  dataIndsAct<-sapply(as.list(tempPop[nInteract==behavTime,indId]),
                      function(x){x=
                        logist(totRBF(rangx,
                                      centers,sigSquar,
                                      as.double(
                                        tempPop[(nInteract==behavTime
                                                 &indId==x)
                                                # &(Quality>0.8&Quality<1)
                                                ,.SD,
                                                .SDcol=grep("WeightAct",
                                                            names(tempPop),
                                                            value = TRUE)
                                                ])),alpha = 0,beta = 1)})
  # par(plt=posPlot(numploty = 4,numplotx = 4,idploty = 2,idplotx = 1))
  par(plt=posPlot(numploty = 1,numplotx = 2,idploty = 1,idplotx = 1,upboundx = 91))
matplot(x=rangx,y=dataIndsAct,type='l',xlab="",ylab="prob. retreat",
          xaxt="s",yaxt="s",lty = 1,
          col=paletteMeans(100)[
            findInterval(tempPop[nInteract==1700,Quality],colorbreaksQual)],
          lwd=3,ylim=c(0,1.1))
text(x = 0.08,y=1.05,"C",cex = 1.5)
par(new=FALSE)
color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "quality",
               numplotx = 20,numploty = 10,idplotx =9,idploty = 2)

SimsDir<-SimsDir<-here("Simulations",paste0(scenario2,"_"))
listTest<-list.files(SimsDir,full.names = TRUE)
indList<-grep("ind",listTest,value=TRUE)
# parameter values from project folder
paramName<-list.files(here("Simulations",paste0(scenario1,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE)
param<-fromJSON(here("Simulations",paste0(scenario1,"_"),paramName[2]))
# fromJSON(paramName[1])
fileId<-1 # choose the replicate

indLearn<-fread(indList[fileId]) # load file
Valpar<-gsub("[[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
nampar<-gsub("[^[:alpha:]]",gsub(".txt","",tail(strsplit(indList[fileId],"_")[[1]],1)),
             replacement = "")
 
gener<-0#tail(indLearn[,unique(time)],2)[2]
nCenters<-param$nCenters
sigSquar<-param$sigSq
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
# nCenters<-5
# interv<-1/nCenters
# centers<-interv*0.5+interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=100)
cexAxis<-2
colorbreaksQual<-seq(0,1,length=100)
tempPop<-indLearn[time==gener]
behavTime<- tempPop[,tail(unique(nInteract),2)[1],by=indId][,min(V1)]
  
rangx<-seq(0,1,by=0.01)
  dataIndsAct<-sapply(as.list(tempPop[nInteract==behavTime,indId]),
                      function(x){x=
                        logist(totRBF(rangx,
                                      centers,sigSquar,
                                      as.double(
                                        tempPop[(nInteract==behavTime
                                                 &indId==x)
                                                # &(Quality>0.8&Quality<1)
                                                ,.SD,
                                                .SDcol=grep("WeightAct",
                                                            names(tempPop),
                                                            value = TRUE)
                                                ])),alpha = 0,beta = 1)})
  # par(plt=posPlot(numploty = 4,numplotx = 4,idploty = 2,idplotx = 1))
  par(plt=posPlot(numploty = 1,numplotx = 2,idploty = 1,idplotx = 2,upboundx = 91)
      ,new=TRUE,xpd=TRUE)
matplot(x=rangx,y=dataIndsAct,type='l',xlab="",ylab="",
          xaxt="s",yaxt="n",lty = 1,
          col=paletteMeans(100)[
            findInterval(tempPop[nInteract==1700,Quality],colorbreaksQual)],
          lwd=3,ylim=c(0,1.1))
text(x = 0.08,y=1.05,"D",cex = 1.5)
text(x = -0.1,y = -0.25,labels = "Badge size")



```


## Results

### Learning in a monomorphic population

We first present the outcome of simulations where we prevented the 
evolution of the badge size (by setting the mutation rate to $0$), 
and assumed that all individuals in the population display either 
an uninformative or informative badge size with respect to their quality. 
These simulations show what type of receiver strategy develops through 
a learning process in such a monomorphic population. 
Figure \ref{fig:model-struc} (C and D) shows 
the receiver strategy developed through learning; panel C is for 
receivers that faced uninformative signals, and D for those facing informative ones. 
The simulations reveal that when learners face uninformative signals (Fig. \ref{fig:model-struc} C), they modify their probability of retreating 
depending on their own quality. 
Individuals with higher quality (red tones) have a 
high probability of attacking after the learning process; 
while individuals of lower quality 
(blue tones) mostly retreat from confrontations; in both cases 
individuals behave equally irrespective of the quality of
their opponent. Thus, learning splits the population of receivers into 
the two classic pure strategies of hawks and doves. 
Given that we assumed a monomorphic population 
with unresponsive reaction norms on the signalling side, the changes 
triggered by learning only affect a small range of badge sizes. Specifically, 
all learning occurred around a badge size of $0.5$, because all
badges in the population are of this size (panel C). 
In contrast, when receivers face informative reaction norms 
on the side of the signaller (Fig. \ref{fig:model-struc} B), 
receivers use information on the badge size of their interacting 
partners to determine whether to retreat or attack. 
The relationship is given by a threshold-like reaction norm, 
where the decision to retreat or to attack depends on the quality of the receiver. 
As expected, the higher the quality of the receiver, the larger the badge size 
of the signaller that triggers a retreat.

```{r handicap,echo=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap="Evolution of the badge size as a handicap mediated by learning. Top panels (A.1 and A.2) show the evolutionary dynamics of the sender code. On the left (A.1), changes in the distribution of values for the intercept of the reaction norm ($\\alpha$); on the right (A.2) changes in the distribution of the slope ($\\beta$). Darker areas of the background correspond to values with high frequency, while orange lines in both panels show the mean of the distribution. X axis in the evolutionary dynamics are given in thousands of generations. Grey lines show the generation time corresponding to the panels below portraing the sender (B) and receiver code(C). In the bottom panels (C1-4), the learned reaction norms correspond to individuals in the population after the interaction round. Color scale indicates quality just as in Fig. \\ref{fig:model-struc}. On the first half of the simulation (B.1 and B.2) the badge size evolves to its minimum value. In the second half, the slope takes positive values and reaction norms evolve to produce an honest signal of quality. Correspondingly, individuals learn to react to badge size by increasing the probability of retreating with increasing badge sizes"  ,warning=FALSE}

scenario<-"betCostEvol3"#"alphaAct"#"nIntGroupNormQual"#
runChoi<-9 #1


# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)
paramName<-list.files(here("Simulations",paste0(scenario,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*5_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))
  # fromJSON(paramName[1])
val<-2
fileId<-val
evolList_runs<-grep(paste0(param$namParam,param$rangParam[2]),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam[2]),
                   indList,value =TRUE)
evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam[2]
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  

# # Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # # to get last interaction: tail(pop[,unique(nInteract)],1)
# #
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 5,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
cexAxis<-0.9

nInd2print<-40

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

adjsYlims<-abs(range(pop[seed==runChoi,alpha])[1]-
                 range(pop[seed==runChoi,alpha])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("topright",legend = c(expression(alpha[s])), bty = "n",cex=cexAxis)
legend("topleft",legend = c("A.1"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alpha]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,beta])[1]-
                 range(pop[seed==runChoi,beta])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = c(expression(beta[s])),cex=cexAxis,bty = "n")
legend("topleft",legend = c("A.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,beta]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(retreat)",rep("",3))
seqYlabDown<-c("Badge size",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge size","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 87),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, nInd2print, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1.1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],
                         colorbreaksQual)],lty = 1,
          cex.axis=cexAxis)
  axis(side = 1,cex.axis=cexAxis,padj = 0)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.2,side = 1 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  legend("topleft",legend = c(paste0("C.",count)),bty = "n",inset = -0.06)
  if(count==1){
   color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "",
               numplotx = 20,numploty = 14,idplotx =3,idploty = 1) 
  }
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 2,
                  lowboundx = 8,lowboundy = 15,
                  upboundy = 99), las=1,new=TRUE)

# meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
# reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
#   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
#          beta = meansClustmp[orderClus==x,betaMean])})
# matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
#          type='l',cex.axis=cexAxis,
#          xlab="",ylab="",ylim=c(0,1.1),lty=1,
#          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1.1),lty=1,col="grey",
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.3,side = 1,las=0)
  legend("topleft",legend =  c(paste0("B.",count)),bty = "n",inset = -0.06)
  }

rm(list=grep("temp",ls(),value = T))


```


### Badges as Handicaps

When we allowed the badge size to evolve ($\alpha$ and $\beta$ 
changed subject to natural selection and genetic drift) and the signal
worked as a handicap (*i.e.* the cost of the badge was inversely 
proportional to the quality of the individual), the sender code evolved often
to produce an honest signal (Fig. \ref{fig:handicap}). The evolution of 
the sender code did not happen immediately after the start of the 
evolutionary process. Instead, the evolutionary dynamics of the reaction 
norm parameters (Fig. \ref{fig:handicap}, A) appeared to involve a set of 
steps. First, the intercept of the reaction norm ($\alpha$) 
describing the relationship between individual quality and badge size evolved to 
higher values, reducing the average badge size in the populations. 
This is because reaction norms segregating in the population are 
flat at the beginning of the simulation; consequently, 
the receiver code does not respond to the badge size 
and larger badges are costlier and do not trigger lower attack 
probabilities. During the first generations, the slope of the sender 
reaction norm ($\beta$)  remains close to the initial value of zero. 
At around generation 4000, the slope evolves toward positive values. 
Positive values in the slope imply that larger badges correlate 
with higher quality (Fig. \ref{fig:handicap} B). Receivers then learn to react to such 
correlation, increasing the probability of retreating when facing 
individuals with larger badges (Fig. \ref{fig:handicap} C). Hence, natural selection 
favours larger values of $\beta$, eventually leading to an evolutionary 
equilibrium in which badge size is an honest signal of quality mediated
by the learned responses of receivers (Fig. \ref{fig:handicap} B.4 and C.4).

The evolutionary trajectory portrayed in figure 
\ref{fig:handicap} A is not the only possible outcome. If the slope of the 
sender reaction norms ($\beta$) evolves, subject to genetic drift,
toward negative values before badges become handicaps, 
receivers never learn to react to the size of badges. 
Thus, badges are only costly and do not provide information about the 
quality of individuals (Fig. \ref{fig:noBadge}) . Eventually, therefore, badges 
disappear from the population. In contrast, when the badge does not 
work as a handicap but instead is cost-free, the evolutionary process 
never leads to the establishment of an honest signal. 
Instead, subject to genetic drift, the 
population either evolves towards the disappearance of the badge or to the 
maximum badge size. In either of these cases reaction norms are flat, so the 
badge does not provide any information about quality. 

### The evolution of polymorphism mediated by learning

The amount of information that agents are able to collect through learning 
over their life time can strongly change the outcome of 
evolutionary dynamics. In the simulations presented in figure 
\ref{fig:handicap} and \ref{fig:noBadge}, 
individuals learned with high speed ($\mathrm{A} = 0.4$) and interacted 
repeatedly along their lifetime (2000 interactions on average). When we reduced 
the number of interactions that individuals experienced over their lifetime to 300 
on average, we saw a drastic increase in the phenotypic and genetic
variation present in the population. Populations start monomorphic 
with a value of zero for both the intercept and
the slope of the reaction norm, and mutations quickly build up a 
normal distribution around the starting 
value. Within the first 2000 generations, the unimodal distribution in 
the intercept ($\alpha$) splits into a bimodal one. 
Later in evolutionary time one of the peaks splits further, 
so at the end of the evolutionary simulation the distribution of the intercept 
($\alpha$) in the population shows three distinct peaks. In the case of $\beta$, the 
peaks in the distribution are not so clear-cut, but the variance of the 
distribution increases over time. These changes in the 
distribution of the parameters of the sender reaction norm 
imply that individuals can generally be classified into three 
distinct types (Fig. \ref{fig:branching}). Two types express a flat reaction 
norm with extreme values for the badge size, meaning that their badge size 
is not informative of their quality, whereas the third type shows 
intermediate badge sizes determined by individual quality 
(Fig. \ref{fig:branching} B.3-4). 
In this simulation, the values of the slope in the informative group
are negative, which implies that badge size correlates negatively with quality. 
Furthermore, the receiver reaction norms developed through learning, particularly those 
of individuals of intermediate quality, respond to the signal of their 
sender type by increasing the probability of retreating from a fight with 
individuals with smaller badges (Fig. \ref{fig:branching}). 

```{r branching,echo=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap= "The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \\ref{fig:handicap}. The top panels (A) show changes in the distribution of values for $\\alpha$ (A.1) and $\\beta$ (A.2) along evolutionary time. Panels below correspond to snapshots of the sender (B) and receiver codes (C). Generation time of the snapshots are indicated by the grey lines in the top panels. The unimodal distribution with which $\\alpha$ (A.1) starts the simulation, quickly turns into a bimondal distribution, and at about 10000 generations it splits further into three modes. These three peaks correspond to three type of reaction norms in panels B.1-3."}

# knitr::include_graphics(here("Simulations","nIntGroupNormQual_","evolDyn7_nIntGroup2000.png"))

scenario<-"betCostEvol4"#"alphaAct"#"nIntGroupNormQual"#
runChoi<-12 #1


# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)

val<-2
fileId<-val

paramName<-list.files(here("Simulations",paste0(scenario,"_")))
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*s0_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))


evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
### Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

adjsYlims<-abs(range(pop[seed==runChoi,alpha])[1]-
                 range(pop[seed==runChoi,alpha])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("topright",legend = c(expression(alpha[s])),
       bty = "n",cex=cexAxis)
legend("topleft",legend = c("A"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alpha]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,beta])[1]-
                 range(pop[seed==runChoi,beta])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = c(expression(beta[s])),cex=cexAxis,bty = "n")
legend("topleft",legend = c("A.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,beta]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 87),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, nInd2print, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1.1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],
                         colorbreaksQual)],lty = 1,
          cex.axis=cexAxis)
  axis(side = 1,cex.axis=cexAxis,padj = 0)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.2,side = 1 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  legend("topleft",legend = c(paste0("C.",count)),bty = "n",inset = -0.06)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  if(count==1){
   color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "",
               numplotx = 20,numploty = 14,idplotx =3,idploty = 1) 
  }
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 2,
                  lowboundx = 8,lowboundy = 15,
                  upboundy = 99), las=1,new=TRUE)

# meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
# reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
#   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
#          beta = meansClustmp[orderClus==x,betaMean])})
# matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
#          type='l',cex.axis=cexAxis,
#          xlab="",ylab="",ylim=c(0,1.1),lty=1,
#          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1.1),lty=1,col="grey",
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.3,side = 1,las=0)
  legend("topleft",legend =  c(paste0("B.",count)),bty = "n",inset = -0.06)
  }

rm(list=grep("temp",ls(),value = T))


```

A limited number of interactions has an effect on the amount of variation in
the evolving parameters when the signal follows the handicap
principle as well. In fig \ref{fig:honestDiv}, we show simulations where individuals 
have on average 300 interactions in their life and the cost of the signal
is proportional to quality, following the handicap principle. 
The evolutionary process leads to a combination of reaction norm parameters where 
there is a positive correlation between the size of the badge and the quality 
of the individual. This relationship, however, is muddled by the fact that there 
are two clusters of values for the intercept ($\alpha$) and slope ($\beta$) 
of the reaction norm in the population. Thus, there are two types 
of reaction norms. One of the types has a steeper slope compared to the other. 
This effect of an increased variance in the trait distribution
is not only triggered by lower number of interactions. 
Larger variances are also found when we assume 
a lower speed of learning (data not shown). This suggest
that limits to the amount of information that individuals acquire through
learning allows the coexistence of different communication strategies within 
a population, something previously shown by @botero_Evolution_2010. 


```{r honestDiv,echo=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap= "The evolution of costly signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \\ref{fig:handicap}. The middle panels show changes in the distribution of values for $\\alpha$ and $\\beta$ along evolutionary time. Panels above and below correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels. Similarly to the evolutionary dynamics in Fig. \\ref{fig:handicap}, first the badge size evolves towards its minimum value. Then, the slope evolves positive values determining an increasing reaction norm. In contrast to \\ref{fig:handicap}, the normal distribution splits up into two modes. This translates into two types of reaction norms assorting in the population where both code for a positive relation between quality and badge size."}

# knitr::include_graphics(here("Simulations","nIntGroupNormQual_","evolDyn7_nIntGroup2000.png"))

scenario<-"betCostEvol4"#"alphaAct"#"nIntGroupNormQual"#
runChoi<-1


# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)
paramName<-list.files(here("Simulations",paste0(scenario,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*5_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[2]))
  # fromJSON(paramName[1])

val<-2
fileId<-val

evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
### Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

adjsYlims<-abs(range(pop[seed==runChoi,alpha])[1]-
                 range(pop[seed==runChoi,alpha])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("topright",legend = c(expression(alpha[s])), bty = "n",cex=cexAxis)
legend("topleft",legend = c("A.1"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alpha]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,beta])[1]-
                 range(pop[seed==runChoi,beta])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = c(expression(beta[s])),cex=cexAxis,bty = "n")
legend("topleft",legend = c("A.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,beta]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 87),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId,nInd2print, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1.1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],colorbreaksQual)],lty = 1,
          cex.axis=cexAxis)
  axis(side = 1,cex.axis=cexAxis,padj = 0)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.2,side = 1 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  legend("topleft",legend = c(paste0("C.",count)),bty = "n",inset = -0.06)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  if(count==1){
   color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "",
               numplotx = 20,numploty = 14,idplotx =3,idploty = 1) 
  }
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 2,
                  lowboundx = 8,lowboundy = 15,
                  upboundy = 99), las=1,new=TRUE)

# meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
# reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
#   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
#          beta = meansClustmp[orderClus==x,betaMean])})
# matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
#          type='l',cex.axis=cexAxis,
#          xlab="",ylab="",ylim=c(0,1.1),lty=1,
#          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1.1),lty=1,col="grey",
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.3,side = 1,las=0)
  legend("topleft",legend =  c(paste0("B.",count)),bty = "n",inset = -0.06)
  }

rm(list=grep("temp",ls(),value = T))
```

### The peaceful, the aggressive and the clever

Our model revealed that the behaviour expressed by naive individuals 
(*i.e.* those who have not yet learned) imposes negative-frequency dependent selection, 
allowing for the build up of genetic variation in reaction norms. 
In the simulations presented so far, we assumed that individuals start with a flat behavioural reaction norm such that 
they escalate fights aggressively with a $0.5$ probability regardless of 
the badge size of the interacting partner. To assess whether such initial 
conditions of the communication system had any role 
in the build up of genetic variation, we ran a series of simulations varying 
the initial conditions of the actor module in the learning model. Specifically,
we let naive individuals have a flat reaction norm with either a 1) low (peaceful),
2) high (aggressive) probability of escalating a fight or 3) a probability 
corresponding to the ESS of the original hawk-dove game 
(which we call clever, due to information and computation necessary to know the ESS). 
Fig. \ref{fig:startCond} shows the distribution of values of the 
intercept ($\alpha$) and slope ($\beta$) evolved in different 
replicates of the simulations. The left-hand side panel, 
corresponding to peaceful naive individuals, is the only one where the 
distribution of values is split in different clusters. That is, most of 
the variation occurs within clusters. In contrast, when naive individuals behave
either aggressively or cleverly, badges evolve toward minimum and maximum 
values, and so the variation is driven mainly by differences among replicates. 
We can make sense of these results by realizing that individuals change their 
naive behaviour quickly in ranges of the badge size that are common in 
the population. In contrast, an individual expressing a rare badge size 
will most likely experience the naive behaviour of their partner. 
When the naive behaviour is peaceful, individuals
with a rare badge size have a fitness advantage. This triggers 
negative-frequency dependent selection and the evolution of different types of 
badge sizes. According to this narrative, situations where individuals 
learn fast and interact repeatedly will diminish the strength 
of frequency dependent selection. 


```{r startCond, echo=FALSE, out.width='100%',fig.align='center',fig.show='hold',fig.cap="The peaceful, the aggressive and the clever. Distribution of values of the intercept $\\alpha$ and slope $\\beta$ for individuals at the end of the evolutionary simulations. The panels show the three different initial conditions for the behaviour of individuals: peaceful, agressive and  \"clever\", see maintext for details. Colours indicate the replicate simulation. In the inset, the resulting reaction norms corresponding to the intecept and slope values for one of the replicates, which replicate is shown is indicated by the line colour. The simulations where individual disply a \"clever\" and agressive behaviour, before the learning process, show all the points of a single replicate cluster together, while in simulation where individuals start being peaceful al replicates show distinct groups of individuals spread throughout the x and y axis. Only in this condition there is a frequency-dependent process that favour diversity in the reaction norm."}

# knitr::include_graphics("../Simulations/initAct_/indVarScatter_.png")

scenario<-"initAct4"

# extSimsDir<-#here("Simulations",paste0(scenario,"_"))
#   paste0("e:/BadgeSims/",scenario,"_")

# Load files -------------------------------------------------------------------

listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
listTest<-grep(".txt",listTest,value = TRUE)

# (listTest<-list.files(extSimsDir,full.names = TRUE))

sdList<-grep("evol",listTest,value=TRUE)
indList<-grep("ind",listTest,value=TRUE)


inds<-rbindlist(lapply(indList,filesScenar,scenario,full.name=TRUE),fill = TRUE)

inds[,diffActWeight:=abs(WeightAct_0-WeightAct_4)]

shortSce<-gsub("[^[:alpha:]]",gsub(".txt","",tail(strsplit(indList[1],"_")[[1]],2)[1]),
               replacement = "")


# Individual variation for different parameters ------------------------------

lastInt<-tail(inds[,unique(nInteract)],2)[1]

popOneInd<-inds[nInteract==lastInt]


# vars<-c("alpha","beta","Badge")
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 5,
#                                 Bsamples =500,iterMax = 500)

# repsScen<-c(12,14,13)
repsScen<-c(5,2,7)

rangx<-seq(0,1,length=50)

list.DataReact<-lapply(popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)],
function(x){
  count<-match(x,popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)])
  tempPop<-popOneInd[get(shortSce)==x&(seed==repsScen[count]&time==max(time))]
  tempPop<-tempPop[,.SD[.N],
                   .SDcol=c(grep("WeightAct",
                                 names(popOneInd),value = TRUE),"Quality","alpha","beta"
                            # ,
                            # "orderClus"),
                   ),
                   by=indId]
  dataIndReact<-sapply(as.list(tempPop[,indId]),
                       function(x){x=
                         sapply(rangx, 
                                function(y)
                                  do.call(logist,
                                          as.list(
                                            c(y,as.double(
                                              tempPop[indId==x,.SD,
                                                      .SDcol=c("alpha","beta")])))))})
  return(dataIndReact)
})



names(list.DataReact)<-
  popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)]




yaxtAll<-c("s","n","n")
xlabAll<-c("",expression(beta[s]),"")
ylabAll<-c(expression(alpha[s]),"","")
titleAll<-c("Peaceful","Aggressive","Clever")
colorAll<-c("dodgerblue2","#E31A1C","green4")
multDiscrPallet[repsScen+1]<-colorAll
plot.new()
for(PAr in popOneInd[,unique(get(shortSce))][c(3,2,1)]){
  count<-match(PAr,popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)])
  # par(plt=posPlot(numploty = 2,idploty = 2,
  #                 numplotx = length(popOneInd[,unique(get(shortSce))]),
  #                 idplotx = count)+c(0,0,-0.05,-0.05),
  #     las=1,new=TRUE,
  #     yaxt=yaxtAll[match(PAr,popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)])],
  #     cex.axis=1.5,new=TRUE)
  # plot(data=popOneInd[(get(shortSce)==PAr&time>max(time)*0.8)&seed %in% 12:15],
  #      alpha~beta,ylab="",
  #      xlab="", pch=20,cex.lab=3,cex.axis=2,las=1,cex=2,xaxt="n",
  #      ylim=range(popOneInd[,alpha])*c(1.05,1.05),
  #      xlim=range(popOneInd[,beta]),
  #      # col=alpha(multDiscrPallet[seed+1],0.3))
  #      col=multDiscrPallet[seed+1])
  # # points(data=popOneInd[(get(shortSce)==PAr&time>max(time)*0.8)&
  # #                         seed == repsScen[count]],
  # #      alpha~beta,col=multDiscrPallet[seed+1],cex=3,pch=20)
  # lines(x=c(0,0),y=range(popOneInd[,alpha]),col="grey",
  #       lwd=2)
  # lines(y=c(0,0),x=range(popOneInd[,beta]),col="grey",
  #       lwd=2)
    
  par(plt=posPlot(numploty = 1,idploty = 1,
                  numplotx = length(popOneInd[,unique(get(shortSce))]),
                  idplotx = count,upboundx = 98,lowboundx = 13)
      +c(-0.03,-0.03,-0.025,-0.025),
      las=1,new=TRUE,yaxt=yaxtAll[count],
      cex.axis=1.5,new=TRUE)
  plot(data=popOneInd[get(shortSce)==PAr&time>max(time)*0.8],alpha~beta,ylab="",
       xlab="", pch=20,cex.lab=cexAxis,cex.axis=cexAxis,las=1,cex=1,
       ylim=range(popOneInd[,alpha])*c(1.1,1.05),
       xlim=range(popOneInd[,beta]),col=multDiscrPallet[seed+1])
  lines(x=c(0,0),y=range(popOneInd[,alpha]),col="grey",
        lwd=2)
  lines(y=c(0,0),x=range(popOneInd[,beta]),col="grey",
        lwd=2)
  mtext(text = xlabAll[count],side = 1,line = 2,cex = cexAxis)
  mtext(text = ylabAll[count],side = 2,line = 2,las=1,cex=cexAxis)
  mtext(text = titleAll[count],side = 3,cex = cexAxis,line = 0.2,
        col = colorAll[count])
  
  points(data=popOneInd[(get(shortSce)==PAr&time>max(time)*0.8)&
                          seed == repsScen[count]],
       alpha~beta,col=multDiscrPallet[seed+1],cex=1,pch=20)

}
for(PAr in popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)]){
  count<-match(PAr,popOneInd[,sort(unique(get(shortSce)),decreasing = TRUE)])

  par(new=TRUE,plt=posPlot(numplotx = 6,numploty = 4,idplotx = count*2-1,
                           idploty = 1,upboundx = 96,lowboundx =11,
                           upboundy = 98)
      +c(0.02,0.02,0.03,0.03),xpd=T,xaxt="s",yaxt="s")

  plot(x=0,y=0,type='l',xlab="", ylab="",ylim=c(0,1),xlim=c(0,1),
       xaxt="n",yaxt="n")
  polygon(x=c(par("usr")[1],par("usr")[1],par("usr")[2],par("usr")[2]),
          y=c(par("usr")[3],par("usr")[4],par("usr")[4],par("usr")[3]),
          col = "white",border=T)
  matlines(x=rangx,y=list.DataReact[[count]],
    #        col = paletteMeans(100)[
    # findInterval(tempPop[,Quality],colorbreaksQual)],
    col=alpha(multDiscrPallet[repsScen[count]+1],1),
    type='l',xlab="",
    ylab="",ylim=c(0,1),lty=1,lwd=2,new=T)
  axis(1,at = c(0,1),tick = TRUE,cex.axis=.7,padj=-1.7)
  axis(2,at =c(0,1),tick = TRUE,cex.axis=.7,hadj=-0.2)
}

```

The action of natural selection and genetic drift on the learning parameters 
do not override the conditions for the evolutionary branching 
[@dieckmann_Dynamical_1996] described earlier. 
Given how crucial naive behaviour is for the branching event 
described earlier, we tested whether evolutionary processes could lead 
key parameters of the learning process toward
values that prevented the genetic variation to build up. 
Figure \ref{fig:learnEvol} shows the dynamics from a set of simulations where we 
allow for the coevolution of the reaction norm as well as key parameters of
the learning module, namely the speed of learning ($A$) and the behavioural 
tendency expressed by individuals prior to learning. Top panels in 
figure \ref{fig:learnEvol} show that there is diversification process in this 
coevolutionary scenario as well. That is because, as is shown in the middle 
panels (B) of the same figure, the learning parameters stay within the range 
necessary for the branching event to take place. The speed of learning 
(Fig. \ref{fig:learnEvol} B.1) stays well above zero throughout the simulation. 
In contrast, the initial behavioral tendency maintain values around zero. 
Thus, evolutionary processes acting on the learning parameters 
did not override the conditions necessary for frequency-dependent selection to boost 
genetic variation. 

## Discussion

We presented a model that integrates learning with 
evolutionary processes. In the model the response of a signal in a 
communication system is mediated by learning processes. Individuals 
throughout their life learn the best way to respond to a quantitative 
trait of their interactive partners. 
We show that the learning process can mediate the evolution 
of an honest signal under the handicap principle
[@grafen_Biological_1990;@zahavi_Mate_1975]. This conclusion is not 
different from classical genetic models, where responses to signals are
innate to the individual. However, unlike classical genetic models,
our model shows that learning can also mediate the origin of a signal 
polymorphism in the absence of the handicap principle. 
Under this polymorphic equilibrium the population consist of distinct 
types of individuals, with honest and dishonest signals. 
The amount of information gathered through learning, as a well as the 
initial conditions of the learning process, were crucial for the emergence
and maintenance of variation in the signalling system. 
Namely the polymorphism required
a limit in the amount of information collected by individuals, as well as
peaceful behaviour in naive individuals. Furthermore, we have shown that the 
conditions leading to the origin of the polymorphism can be reached when evolutionary
processes are able to change the parameters of the learning system (the speed of
learning and the naive behaviour). 

Associative learning is a powerful mechanism to learn about the world, about 
social partners, and about one's abilities in a particular social context. 
By associating cues and signals with fitness-relevant outcomes, individuals
collect information allowing them to improve their reproductive potential. 
This is evident in classical situations such as when animals learn to avoid 
food that makes them sick or use environmental cues to find food. 
More recent theoretical work has highlighted the potential role that associations 
can have in social contexts such as hierarchy formation 
[@leimar_Evolution_2021;@leimar_Reproductive_2022;@leimar_Effects_2022]
and cooperation [@leimar_Learning_2019;@dridi_Learning_2018]. In these 
examples, models involve individuals who use various sources of reward to make
adaptive choices. Here we extended this logic to the 
evolution of a communication system mediated by a signal of quality. 
Previous evolutionary models of badges of status assumed that individuals 
responded using a behavioural reaction norm, where the opponent's badge 
and the individual's own quality determined behaviour [@botero_Evolution_2010]. 
However, there is no clear mechanism justifying the assumption that 
individuals inherently know their own quality, particularly relative to their peers.
In our model, individuals not only learn about the quality signal, but also 
learn about their own quality relative to others. This can be seen in
our simulations in the responses developed by individuals of different quality. 
Because individuals learn about their own quality, 
even in the absence of an honest signal, they are able to make more 
adaptive choices. Specifically, lower quality individuals refrained more from
escalating fights, while higher quality took over resources they 
could win in a fight. Thus, the learning process we modelled and the 
response it mediates has fitness relevant consequences even in the 
absence of an honest signal. This was confirmed by simulations where 
the speed of learning was allowed to change subject to evolutionary processes. 
In those simulations, natural selection always
maintained learning rates above zero. Our simulations however, 
only captured the effect of selection mediated by the social 
game on the learning parameters. For example, We did not account for 
potential costs of faster updating, or improved performance in foraginf tasks. 
Inter-specific variation in cognitive abilities mediated by environmental 
differences has been reported elsewhere
[@sonnenberg_Natural_2019]. If such variation is partly mediated by changes 
that affect general cognitive processes, like the speed of learning,
they could impact the outcome of communication systems like the one we modelled. 

Learning processes collecting information on a population wide level promote 
frequency dependent selection. Frequency-dependence triggered by 
learning process was highlighted by a 
classic study where live predators drive the evolution
of *in silico* polymorphic prey [@bond_Visual_2002]. 
The key to the evolution of prey polymorphism is 
that predators are better able to discriminate prey from the background, 
when preys are frequently encountered.
Thus, prey morphs found in low frequency in a population have a selective advantage. 
An analogous process emerged in our simulations, in which the 
receiver individual learned the appropriate response towards individuals 
with a common badge size in the population. When an individual has a
rare badge size it can be favoured or unfavoured by the naive behaviour. When 
the naive behaviour is peaceful, polymorphism is promoted, whereas when the naive 
behaviour is aggressive polymorphism is prevented. These two situations show how
behaviours dependent on learning processes seem to generally trigger 
frequency-dependent selection. That is because the learning algorithm 
collects more information on the more frequent values of the the trait distribution. 
For the sake of simplicity we modelled a single dimension (badge size), 
however the frequency-dependen effect of learning could potentially be 
more important when individuals learn from multidimensional signals. 


Originally, @rohwer_Social_1975a proposed the idea that certain phenotypic 
traits could be used by animals as status signals or signals of quality. 
This idea has been tested repeatedly on the dark patches of some bird species, 
such as the bibs exhibited by species in various families, particularly sparrows.
So, are bibs real signals of quality? if so are they handicaps or 
badge of status? From an evolutionary perspective it is unclear how the stability of 
status signals can be maintained. One option is that honesty is 
maintained by the cost of signal production; that is, when the signal works as a 
handicap [@botero_Evolution_2010;@grafen_Biological_1990;@johnstone_Badges_1993]. 
Alternatively, if the signal does not carry production costs inversely 
proportional to quality it may work as a convention.
In this later case, honesty is presumably maintained by the aggressive 
reaction of receivers when the convention is 
broken [@enquist_Signaling_2010;@tibbetts_Socially_2004]. Our model, portraits 
both kinds of signals, although with some nuances. When we assume the handicap 
principle, simulations result in the evolution of honest signals mediated by the
learned response. When we assume cost-free signal, under certain conditions the
learned response mediated the emergence of phenotypic variation in the signal. 
This variation, particularly in the mid-range of the distribution facilitates 
the establishment of a convention, in this range individuals respond 
appropriately to the trait of their peer. The house sparrow (*Passer domesticus*) 
has been a text-book example of badges of status. According to the prevailing narrative the bib size in males is a signal of dominance rank and quality.
However, a recent meta-analysis called into question 
this narrative by showing that the effect size of the association between bib 
size and dominance rank is small and uncertain [@sanchez-tojar_Metaanalysis_2018a]. 
A simple correlation between bib size and dominance rank
(and quality) is expected under the handicap principle, as it is shown here and 
elsewhere [@botero_Evolution_2010;@grafen_Biological_1990;@johnstone_Badges_1993].
However, the branching event presented here shows that conventions can 
emerge in a less straightforward way when mediated by learning processes. These 
nuances could potentially make sense of seemingly contradictory evidence on the 
correlation between plumage traits and quality. For example, conventions in our
simulations arose only in an intermediate range of the signalling trait, yet 
empirical studies seeing for correlations between phenotypic traits and quality 
are often performed across the whole range of variation. 

The focus of theoretical and empirical work on communication systems, 
and particularly badges of status, is often explaining the presence and 
absence of certain morphological traits within populations. However, the 
morphological traits hypothesized to play a role in communication systems 
regularly present interesting patters at the phylogeographic
level. One example of this is the leapfrog pattern [@remsen_High_1984],
whereas a certain morphological trait alternates its presence and 
absence in a set of geographically adjunct populations. Patterns of 
molecular variation along geographic clines suggest that natural selection 
is involved in the emergence of these patterns 
[@cadena_Testing_2011;@cadena_Evolutionary_2007]. However, it is unclear what 
type of ecological process is behind these selective regimes. 
Given the wide variation in outcomes found in our model, that depend on both 
stochasticity and cognitive parameters, we think variation in cognition and 
learning in particular could provide some explanatory power in this respect. 
One first step, for example is to evaluate the way individuals in different 
populations respond to novel traits [@avendano_Territorial_2021]. 

We have presented here an evolutionary model where the evolution of a 
communication system is mediated through the learned responses of
receivers. This is a novel way to understand the evolution of communication
that integrates classical cognitive process in learning with 
evolutionary explanations of communication. This approach contributes to the 
further integration of proximate and ultimate explanations in behavioral and 
evolutionary biology. 

# Supplementary material

```{=tex}
\beginsupplement
```

```{r noBadge,echo=FALSE,warning=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap= "The evolution of cheap signals. Portrait of the evolutionary dynamics of the sender code with snapsots of both sender and receiver codes just as in fig. \\ref{fig:handicap}. The middle panels show changes in the distribution of values for $\\alpha$ and $\\beta$ along evolutionary time. Panels above and below correspond to snapshots of the sender and receiver codes, respectively, generation time of the snapshots are indicated by the grey lines in the middle panels."}

# knitr::include_graphics(here("Simulations","betCostEvol1_","evolDyn5_betCost5.png"))

scenario<-"betCostEvol3"#"alphaAct"#"nIntGroupNormQual"#
runChoi<-12 #1


# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)

val<-2
fileId<-val

paramName<-list.files(here("Simulations",paste0(scenario,"_")))
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*s0_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[1]))


evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam[1]
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
runChoi<-9#3
## Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))

popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", "meanInitAct", "sdInitAct",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 # "meanInitAct","sdInitCrit","sdInitAct",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.5,0))

adjsYlims<-abs(range(pop[seed==runChoi,alpha])[1]-
                 range(pop[seed==runChoi,alpha])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1,side = 2,las=0)
legend("topright",legend = c(expression(alpha[s])),bty = "n",cex=cexAxis)
legend("topleft",legend = c("A.1"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alpha]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 15,
                upboundy = 99),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,beta])[1]-
                 range(pop[seed==runChoi,beta])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = c(expression(beta[s])),cex=cexAxis,bty = "n")
legend("topleft",legend = c("A.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,beta]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 87),
      xaxt="s",las=1,new=TRUE)
  weightsAct<-as.double(evol[(time==unique(time)[genC])&seed==runChoi,.SD,
                             .SDcols=grep("WeightAct",
                                          names(evol),value = TRUE)])
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId, nInd2print, replace = FALSE)
  
  dataIndAct<-sapply(as.list(tempPop[indId %in%randInds,indId]),
                     function(x){x=
                       logist(totRBF(rangx,
                                     centers,sigSquar,
                                     as.double(
                                       tempPop[indId==x,.SD,
                                               .SDcol=grep("WeightAct",
                                                           names(tempPop),
                                                           value = TRUE)
                                               ])),alpha=0,beta = 1)})
  matplot(x=rangx,y=dataIndAct,
          yaxt=seqYax[count],ylab="",xlab="",type="l",cex.lab=cexAxis,
          lwd=2,xaxt="n",ylim=c(-0.06,1.1),col = paletteMeans(100)[
            findInterval(tempPop[indId %in%randInds,Quality],
                         colorbreaksQual)],lty = 1,
          cex.axis=cexAxis)
  axis(side = 1,cex.axis=cexAxis,padj = 0)
  mtext(text = seqYlabUp[count],cex = cexAxis,line = 1.7,side = 2,las=0)
  mtext(text = seqXlabUp[count],cex = cexAxis,line = 1.2,side = 1 ,las=0)
  lines(logist(totRBF(rangx,centers,sigSquar,weightsAct),alpha=0,beta=1)~rangx,
        col=1,lwd=3,lty=2)
  legend("topleft",legend = c(paste0("C.",count)),bty = "n",inset = -0.06)
  # text(x=0.5,y=-0.015,labels = paste0("time=",unique(evolStats$time)[genC]/1000),
  #      cex=2)
  if(count==1){
   color.bar.aeqp(paletteMeans(100),min =min(colorbreaksQual),
               max = max(colorbreaksQual),nticks = 3,
               cex.tit = 0.9,title = "",
               numplotx = 20,numploty = 14,idplotx =3,idploty = 1) 
  }
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 2,
                  lowboundx = 8,lowboundy = 15,
                  upboundy = 99), las=1,new=TRUE)

# meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
# reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
#   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
#          beta = meansClustmp[orderClus==x,betaMean])})
# matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
#          type='l',cex.axis=cexAxis,
#          xlab="",ylab="",ylim=c(0,1.1),lty=1,
#          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1.1),lty=1,col="grey",
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.5,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.3,side = 1,las=0)
  legend("topleft",legend =  c(paste0("B.",count)),bty = "n",inset = -0.06)
  }

rm(list=grep("temp",ls(),value = T))

```

```{r learnEvol,echo=FALSE,out.width='100%',fig.height=6,fig.align='center',fig.show='hold',fig.cap= "Coevolutionary dynamics of both the signal reaction norm and key parameters of the learning module. Dynamics are portrait as changes in the distribution of values assorting in the population. In the top panels (A)  evolutionary dynamics of the sender code; A.1 shows the intercept and A.2 the slope of the logistic reaction norm. Both of these parameters experience evolutionary branching, whereby in the end three distincs types are assorting in the population. The middle panels (B) show changes in the distribution of values for the speed of learning ($A$;B1) and the behavioural tendency before learning ($y_z^0$; B2) along evolutionary time. The speed of learning does not change drastically, it maintains values well above 0 (black line). In contrast, the initial behavioural tendency does not evolve away from 0 (black line). Both of these conditions favour evolutionary branching in the signal reaction norm. Bottom Panels correspond to snapshots of the sender code. Generation time of the snapshots are indicated by the grey lines in the top and middle panels."}

# knitr::include_graphics(here("Simulations","nIntGroupNormQual_","evolDyn7_nIntGroup2000.png"))

scenario<-"betCostalphaL"#"alphaAct"#"nIntGroupNormQual"#
runChoi<-13


# Load files -------------------------------------------------------------------
# Project folder
listTest<-list.files(here("Simulations",paste0(scenario,"_")),full.names = TRUE)
evolList<-grep("evolLearn",listTest,value=TRUE)
indList<-grep("indLearn",listTest,value=TRUE)
paramName<-list.files(here("Simulations",paste0(scenario,"_")))
# paramName<-list.files(extSimsDir,full.names = TRUE)
paramName<-grep(".json",paramName,value=TRUE) %>% grep(pattern = "*0_*",value = TRUE)

param<-fromJSON(here("Simulations",paste0(scenario,"_"),paramName[1]))
  # fromJSON(paramName[1])

val<-1
fileId<-val

evolList_runs<-grep(paste0(param$namParam,param$rangParam),
                    evolList,value =TRUE)
indList_runs<-grep(paste0(param$namParam,param$rangParam),
                   indList,value =TRUE)


evol<-do.call(rbind,lapply(evolList_runs,fread))
pop<-do.call(rbind,lapply(indList_runs, fread))
  
Valpar<-param$rangParam
  
nampar<-param$namParam
  
nCenters<-param$nCenters
sigSquar<-param$sigSq
  
# Aesthetic paremeters ---------------------------------------------------------

## Calculate clustering for all generations  -------------------
  
### Choose which interaction to visualize
# lastInt<-tail(pop[,unique(nInteract)],4)[2]
# # pop[,max(nInteract),by=.(seed,time)][,min(V1)]
# # to get last interaction: tail(pop[,unique(nInteract)],1)
# 
# popOneInd<-pop[nInteract==lastInt]
# vars<-c("alpha","beta","Badge")
# # ,"Badge",grep("WeightAct",names(popFinal),value = TRUE))
# popOneInd$idClust<-get_clusters(popOneInd,vars,k.max = 10,
#                                 Bsamples =500,iterMax = 500)
# clusSummary<-popOneInd[,.(meanAlph=mean(alpha),meanBet=mean(beta)),
#                        by=.(idClust,time,seed)]
# clusSummary[,orderClus:=0]
# invisible(capture.output(
#   for(i in 1:dim(clusSummary)[1]){
#   set(clusSummary,i,6L,match(clusSummary[i,meanAlph],
#         sort(clusSummary[time==clusSummary[i,time]&
#                            seed==clusSummary[i,seed]]$meanAlph)))
# }
# ))
# 
# 
# popOneInd<-merge(popOneInd,clusSummary,all = TRUE)
# 
# 
# fwrite(popOneInd,file = here("Simulations",paste0(scenario,"_"),
#                              paste0("popOneInd",nampar,Valpar,".txt")))
# 
popOneInd<-fread(here("Simulations",paste0(scenario,"_"),
                             paste0("popOneInd",nampar,Valpar,".txt")))

# Get stats from the evolutionary simulations ----------------------------------
# names(evol)

cols<-c("freqGenHawks","freqGenDove",  "freqGenEval",  #"freqGenLearn",
        "freqFenHawks", "freqFenDoves", "freqHH", "freqHD", "freqDD", "meanCue",
        "meanAlpha", "meanBeta",
        #"meanFit", #"meanInitCrit", "sdInitCrit", 
        "meanInitAct", "sdInitAct", "meanAlphaLearn","sdAlphaLearn",
        "WeightAct_0","WeightCrit_0",
        "WeightAct_1","WeightCrit_1","WeightAct_2",  "WeightCrit_2",
        "WeightAct_3",  "WeightCrit_3", "WeightAct_4",  "WeightCrit_4",
        "WeightAct_5","WeightCrit_5")

my.summary<- function(x) list(mean = mean(x), lowIQR = fivenum(x)[2], 
                              upIQR = fivenum(x)[4])

evolStats<-evol[, as.list(unlist(lapply(.SD, my.summary))), 
                .SDcols = cols,by=time]

## Plot mean and IQRs among individuals for each replicate --------------------

# get the trajectories for individual runs
traitsTrajs<-dcast(evol,time~seed,
                   value.var = c("meanAlpha","meanBeta",
                                 #"meanInitCrit",#"meanFit",
                                 "meanInitAct", "sdInitAct",
                                 "meanAlphaLearn","sdAlphaLearn",
                                 "sdAlpha","sdBeta","freqHH",
                                 "freqHD","freqDD"))

finReps<-evol[time==max(time),seed]

genstoPrint<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]

# Evolutionary dynamics for alpha - intercept of the sender --------------------------------------------
par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 92,upboundx = 92),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.8,0))

adjsYlims<-abs(range(pop[seed==runChoi,alpha])[1]-
                 range(pop[seed==runChoi,alpha])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alpha",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alpha])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1.8,side = 2,las=0)
legend("topright",legend =
      expression(paste("intercept (",alpha[s],")")),bty = "n",cex=cexAxis)
legend("topleft",legend = c("A.1"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alpha]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlpha_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=5)

par(plt=posPlot(numploty = 3,idploty = 3,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 92,upboundx = 92),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,beta])[1]-
                 range(pop[seed==runChoi,beta])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "beta",nbins = 10,pal = pal_dist,
         nlevels=10,cexAxis = 1.5,xlab="",ylab = "",xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,beta])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = expression(paste("Slope (",beta[s],")")),cex=cexAxis,
       bty = "n")
legend("topleft",legend = c("A.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,beta]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanBeta_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# Evolutionary dynamics for the speed of learning-------------------------------

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2,idplotx = 1,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 92,upboundx = 92),
    mfrow=c(1,1), xaxt="s",las=1,mgp=c(2,0.8,0),new=TRUE)


adjsYlims<-abs(range(pop[seed==runChoi,alphaLear])[1]-
                 range(pop[seed==runChoi,alphaLear])[2])*c(-0.1,0.2)
            

evolDist(indData = pop[seed==runChoi],variable = "alphaLear",nbins = 10,
         pal = pal_dist,
         nlevels=10,cexAxis = cexAxis,xlab="",ylab = "",xaxt="n",yaxt = "s",
         ylim = range(pop[seed==runChoi,alphaLear])+adjsYlims,
         xlim=c(0,22000))

mtext(text = "Trait value",cex = cexAxis,line = 1.8,side = 2,las=0)
legend("topright",legend = "Speed of learning", bty = "n",cex=cexAxis)
legend("topleft",legend = c("B.1"),cex=cexAxis,bty = "n",inset = -0.06)


axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

# black line showing the value 0 as a benchmark 

lines(x = pop[,range(time)],y=c(0,0))

# grey lines to show the generations shown in the upper and lower panels

matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,alphaLear]),4),nrow = 2),
         lty=1,col = "grey",lwd=2)

# Variation among individuals

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanAlphaLearn_"),runChoi)],         col=colGenesLin,lty = 1,type="l",lwd=5)

## Plot evolutionary dynamics of the naive behaviour of inds -------------------

par(plt=posPlot(numploty = 3,idploty = 2,numplotx = 2, idplotx = 2,
                lowboundx = 8, title = TRUE,lowboundy = 10,
                upboundy = 92,upboundx = 92),
    xaxt="s",las=1,new=TRUE)

adjsYlims<-abs(range(pop[seed==runChoi,initAct])[1]-
                 range(pop[seed==runChoi,initAct])[2])*c(-0.1,0.2)

evolDist(indData = pop[seed==runChoi],variable = "initAct",nbins = 10,
         pal = pal_dist, nlevels=10,cexAxis = 1.5,xlab="",ylab = "",
         xaxt="n",yaxt = "n",
         ylim = range(pop[seed==runChoi,initAct])+adjsYlims,xlim=c(0,22000))
legend("topright",legend = c("Naive behavioural tendency"),cex=cexAxis,
       bty = "n")
legend("topleft",legend = c("B.2"),cex=cexAxis,bty = "n",inset = -0.06)
# grey lines to show the generations shown in the upper and lower panels
matlines(x=matrix(rep(evolStats[genstoPrint,time],each=2),nrow=2),
         y=matrix(rep(range(pop[seed==runChoi,initAct]))),
                      lty=1,col = "grey",lwd=2)

axis(side=1,padj = -2,at = axTicks(1)[2:(length(axTicks(1)))],
     labels = axTicks(1)[2:(length(axTicks(1)))]/1000,cex.axis=cexAxis,
     hadj = 1.3,line = -0.5,tick = FALSE)

axis(4,cex.axis=cexAxis)

matlines(x=traitsTrajs[,time],
         y=traitsTrajs[,.SD,
                       .SDcol=paste0(c("meanInitAct_"),runChoi)],
         col=colGenesLin,lty = 1,type="l",lwd=4)

# black line showing the value 0 as a benchmark 

lines(x = pop[,range(time)],y=c(0,0))


# Choose time range
gen2plot<-round(seq(1,length(unique(evolStats$time)),length.out = 5))[2:5]
# Plor the actor
seqYax<-c("s",rep("n",3))
# seqYlabUp<-c("Badge",rep("",3))
seqYlabUp<-c("P(dove)",rep("",3))
seqYlabDown<-c("Badge",rep("",3))
seqXlabDown<-c("","Quality","")#paste0("seed: ",runChoi))
seqXlabUp<-c("","Badge","")
rangQual<-seq(0,1,length.out = 50)
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
rangx<-seq(0,1,length=50)
count<-0
for(genC in genstoPrint){
  count<-count+1
  par(plt=posPlot(numplotx = 4,numploty = 3,idplotx = count,idploty = 1,
                  lowboundx = 8, title = TRUE,lowboundy = 10,
                  upboundy = 92,upboundx = 92),
      xaxt="s",las=1,new=TRUE)
  tempPop<-popOneInd[(time==unique(time)[genC]&seed==runChoi),.SD[.N],
               .SDcol=c(grep("WeightAct",
                             names(evol),value = TRUE),"Quality","alpha","beta",
                        "orderClus"),
               by=indId]
  
  randInds<-sample(tempPop$indId,nInd2print, replace = FALSE)
  
  
# meansClustmp<-tempPop[,.(alphaMean=mean(alpha),betaMean=mean(beta)),by=orderClus]
# reacNormpClust<-sapply(meansClustmp[,orderClus], function(x){
#   logist(rangx,alpha = meansClustmp[orderClus==x,alphaMean],
#          beta = meansClustmp[orderClus==x,betaMean])})
# matplot(x = rangx,y = reacNormpClust,col=colRuns[meansClustmp$orderClus],
#          type='l',cex.axis=cexAxis,
#          xlab="",ylab="",ylim=c(0,1.1),lty=1,
#          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  reacNormInd<-sapply(tempPop[indId %in%randInds,indId], function(x){
    logist(rangx,alpha = tempPop[indId==x,alpha],
           beta = tempPop[indId==x,beta])})
  matplot(x = rangx,y = reacNormInd,
          type='l',cex.axis=cexAxis,
          xlab="",ylab="",ylim=c(0,1.1),lty=1,col="grey",
          lwd=2,yaxt=seqYax[count],xaxt="s",cex.lab=cexAxis)
  mtext(text = seqYlabDown[count],cex = cexAxis,line = 1.9,side = 2,las=0)
  mtext(text = seqXlabDown[count],cex = cexAxis,line = 1.7,side = 1,las=0)
  legend("topleft",legend = c(paste0("C.",count)),bty = "n",inset = -0.06)
  }

rm(list=grep("temp",ls(),value = T))
```


\newpage
\small


| Symbol |      Description |
|:-------:|:--------------------------------|
| $Q_i$ | Quality of individual $i$ |
| $N$   | Population size |
| $n$   | Number of interactions as the focal individual |
| $\sigma$ | Standard deviation of the truncated normal distribution from which quality is drawn |
| $B_i$ | Badge size of individual $i$ |
| $\alpha_i$ | Intercept of the badge size reaction norm for individual $i$ |
| $\beta_i$  | Slope of the badge size reaction norm for individual $i$ |
| $\mu$     | Mutation rate | 
| $\sigma_\mu$ |Standard deviation of the normal distribution from which quality is mutations are drawn |
| $s_i$ | Survival probability of individual $i$ |
| $k_1$ | Intercept of the survival function |
| $k_2$ | Slope of the survival function |
| $V$   | Value of the contested resource |
| $C$   | Cost of an escalated fight |
| $p_{ij}$ | Probability that individual $i$ wins an escalated fight against individual $j$ |
| $k_3$ | Parameter determining how important is quality defining the probability of wining |
| $\delta$ | Prediction error |
| $A$ | Speed of learning | 
| $c$ | Number of feature centres |
| $b_z$ | badge size for feature centre $z$  | 
| $\hat{R}$ | Reward estimate | 
| $x_z$ | weight of feature centre $z$ on the reward estimation |
| $\theta$ | width of the generalization function | 
| $q_{ij}$ | probability that individual $i$ retreats from a fight in an encounter with individual $j$ |
| $y_z$ | weight of feature centre z on the tendency to retreat |
| $w_i$ | total pay-off of individual $i$ |
| $w_0$ | base line pay-off |

Table: Notation of the model parameters and variables





## References