---
title: |
  | Learning can mediate both badges of status and individual recognition
author:
- Andrés E. Quiñones^[Laboratorio de Biología Evolutiva de Vertebrados, Departamento
  de Ciencias Biológicas, Universidad de los Andes, Bogotá, Colombia]
- C. Daniel Cadena*
- Olof Leimar^[Department of Zoology, Stockholm University, Stockholm, Sweden]
- Redouan Bshary^[Institute of Biology, University of Neuchâtel, Neuchâtel, Switzerland]
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  word_document: default
fig_caption: yes
fontsize: 12pt
header-includes:
- \usepackage{float}
- \usepackage{setspace}\doublespacing
- \linespread{2}
- \usepackage{lineno}
- \linenumbers
keep_tex: yes
keywords: My keywords
linespread: 2
bibliography: ../SocialMorphCue.bib
spacing: double
style: C:\Users\andre\Zotero\styles\elife.csl
abstract: Adaptive behavioural responses often depend on qualities of the interacting partner of an individual. For example, when competing for resources, an individual might be better off escalating fights with individuals of lower quality, while restraining from fighting individuals of higher quality. Communication systems involving signals of quality allow individuals to reduce uncertainty regarding the fighting ability of their partners and make more adaptive behavioral decisions. However, dishonest individuals can destabilize such communications systems. One open question is whether cognitive mechanisms, such as learning, can maintain the honesty of signals, thus, favoring their evolutionary stability. Here we present evolutionary simulations, where individuals can produce a signal proportional to their quality; and learn along their lifetime the best response to the signal emitted by their peers. In these simulations, learning on the receiver side can mediate the evolution of signals of quality on the sender side. When the cost of the signal is proportional to the quality of the sender populations are only composed of honest signalers. When the cost is not proportional to the quality of the signaler, the population is composed of both honest and dishonest signalers. We argue that learning can be a general cognitive mechanism playing a role in a wide range of communication systems.  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(png)
require(here)
source(here("../R_files/posPlots.r"))
source(here("AccFunc.r"))
```

## Introduction 

The outcome of interactions with con-specifics is an important 
determinant of fitness in social animals. Irrespective of whether such interactions are cooperative 
or competitive, the actions of interacting individual partly determine 
their reproductive success. However, the best action for an individual 
might vary depending on its condition, and that of individuals with whom it interacts. 
Thus, acting based on information about interacting 
partners is typically adaptive [@quinones_Negotiation_2016].
Acquiring this information, however, is far from trivial. 
In some cases, interacting partners (signalers) might be 'willing' to provide accurate
information, but in others it might be in their own interest to conceal information
[@johnstone_Recognition_1997], or to provide erroneous information [@johnstone_Badges_1993]. 
Typically, in a given context, some proportion of individuals will beenefit 
from broadcasting accurate information, whereas others will benefit from hiding it. 
Take, for instance, an interaction between two individuals where one can help the other. 
Given some costs and benefits, a potential donor will be interested in helping 
related individuals. Therefore, for a relative of the donor, 
broadcasting kinship is advantageous, whereas for an unrelated individual, 
concealing the lack of kinship is better. Similar scenarios may apply to 
many aspects of social life such as finding mates, feeding offspring, dominance relationships 
and aggressive interactions [@bradbury_Principles_2011; @moller_Female_1988; @tibbetts_Socially_2004].
In dominance and aggressive interactions, an important piece of
information to guide individual actions is interacting partners fighting ability, 
which is sometimes referred to as Resource Holding Potential (RHP) [@parker_Assessment_1974], 
or simply as quality. Responsiveness to the quality of the 
partner is central to communication systems such as badges of status, 
where an arbitrary signal conveys quality [@johnstone_Badges_1993; @rohwer_Social_1975a]. 
Here, arbitrary refers to a signal which is not ontogenetically correlated with quality. 
Badges of status can be evolutionarily stable whenever the signal imposes a fitness cost which 
decreases with the quality of an individual  [@botero_Evolution_2010; @johnstone_Badges_1993], 
such that it's no longer in the interest of low quality individuals to fake quality by producing a
large badge. The cost of honest signals is a more general principle of communications systems, 
usually referred to as the handicap principle [@grafen_Biological_1990; @zahavi_Mate_1975]. 
An alternative system of communication is one where individuals gather and store information
about each individual they interact with. This information should reliably allow 
each individual to react adaptively to the quality difference with their opponent. 
This type of assessment strategy is often referred to as individual recognition [@whitfield_Plumage_1986]. Evidently, individual recognition is restricted to 
certain interaction structures, because individuals are limited by cognitive 
abilities in how many individuals they can recognize and track. Thus, individual 
recognition and badges of status have been assumed to be alternative communication 
systems fulfilling the same function, reducing the costs of aggression in contests over resources. 
Furthermore, badges of status and individual recognition are expected to evolve under different interaction structures, badges of status under global interactions and individual recognition
under local interactions [@dale_Signaling_2001; @sheehan_There_2016].  There is no clear cut reason, however, why both systems could not work together, in fact there is some evidence
that they can [@chaine_Manipulating_2018].

A somewhat ignored component of communication systems, in the context of aggressive 
interactions, is the cognitive aspects of the receiver module. Communication systems
relying on badges of status are often assumed to be cognitively less demanding 
than those involving individual recognition [@sheehan_There_2016] because in the latter, 
individuals need to store identity and associated information about each individual 
they interact with. In contrast, the response towards badges of status is often 
thought to be innate [@botero_Evolution_2010; @johnstone_Badges_1993], with 
individuals using their own quality and the opponent's badge to determine whether to aggressively
engage in contest. An alternative view of badges of status is that individuals learn 
to react to them based on their experiences [@guilford_Receiver_1991], which would imply that receivers must learn to associate signals with the fighting ability of bearers just as in systems based on individual recognition. In cases where badges of status vary quantitatively (e.g. in size or intensity), fighting ability may increase monotonically with attributes of the badge and would be reinforced by every interaction, so in principle learning would be faster than in systems involving individual recognition in which the association between signals and their meaning would vary depending on the interacting partners. In any case, learning could be a central cognitive mechanism in both types of communication systems, but the role of learning in these contexts has not been thoroughly explored in the empirical nor theoretical literature. 

Associative learning is a key cognitive mechanism that allows individuals to associate
rewards with environmental stimuli and thus behave adaptively [@staddon_Adaptive_2016]. 
Associative learning exists in all major vertebrate taxa, and in many invertebrates [@heyes_Simple_2012;@macphail_Brain_1982; @staddon_Adaptive_2016; @behrens_Associative_2008].
Theory has shown that natural selection favors these associations in complex environments
where conditions are hard to predict [@dridi_Environmental_2016]. Besides its wide taxonomic and
ecological relevance, associative learning is a flexible cognitive mechanism whose underpinings
show interspecific variation [@enquist_Power_2016; @quinones_Reinforcement_2019], so 
presumably, it has been modified by natural selection. Despite all this, associative 
learning is not often included in the narrative of evolutionary explanations of 
behavioural patterns [@fawcett_Exposing_2013; @kamil_Optimal_1983; @mcauliffe_Psychology_2015]. 
Computational models of evolution can play an important role to overcome the 
lack of integration between learning and evolution. Reinforcement learning theory 
encompases a series of computational methods inspired on the psychological and 
neurological mechanisms of associative learning [@sutton_Reinforcement_2018].
These set of algorithms allow the implementation of biologically realistic problems, 
capturing the essence of learning processes [@frankenhuis_Enriching_2018; @quinones_Reinforcement_2019]. 
Furthermore, these algorithms can be embedded in evolutionary simulations to 
generate theoretical predictions of the effect that learning can have in behavioral
evolution [@leimar_Learning_2019].

In here we present an evolutionary model where individuals use associative learning 
to develop a tendency to behave aggressively (or  peacefully), in the context of
competition over resources, depending on the quantitative morphological trait 
they perceive in their opponent (badge). Over evolutionary time individuals evolve
the size of their badge, and whether it depends on their quality. Under this simple
set up, individuals can use the badge both as a signal of quality (BS)
or as a trait for IR. We use the model to assess under what conditions of interaction
structure we expect different communication signals to evolve. 

## The model

We model the evolution of signals of quality in the context of agonistic interactions by 
modelling a population of haploid individuals with non-overlaping generations. 
Individuals are born every generation with a level of quality ($Q_i$, where $i$ is subscript 
of the population vector of size $N$) given by a number between zero and one, which is drawn 
from a truncated normal distribution $N(\mu = 0.5,\sigma)$. 
An individual with quality $0$ has the lowest RHP, 
while an individual with quality $1$ has the highest. As part of development processes
individuals produce a morphological signal (badge), the size of such depends on their level 
of quality according to a reaction norm given by $B_i=1/(1+e^{\alpha_i-\beta_i Q_i})$;
where are $\alpha_i$ and $\beta_i$ are individual specific traits that determine 
the shape of the reaction norm. Variation in $\alpha_i$ and $\beta_i$ means individuals
can have either uninformative (flat) or informative (logistic) norms; but the size of 
the signal is constrained between zero and one. We assume different values of these traits 
are given by different alleles, and are inherited from mother to offspring. 
Unless, with a small probability, mutation changes the allelic value of the offspring.
After birth individuals go through a round of viavility selection. 
Individual specific survival probability is given by $s_i=1/(1+e^{-k_1-k_2(Q_i-B_i)}$. 
Where $k_1$ and $k_2$ are parameter values determining the shape of the survival function. 
Importantly, if $k_2=0$ survival probability is independent of quality, while if $k_2>0$
lower quality individuals pay a higher price for s similar-sized badge, fulfilling the core assumption 
of the handicap principle [@botero_Evolution_2010; @grafen_Biological_1990; @johnstone_Badges_1993].

Individuals who survive engage in a series of pairwise interactions 
where they compete for resources. Individuals in real populations typically do not 
interact at random nor with all individuals in the population [@kurvers_Evolutionary_2014]. 
In order to allow for non-random interactions, we use the population vector to define 
an interaction neighborhood for the focal individual, the size of the neighborhood (g) 
is the number of positions around the focal´s from which the interactant is drawn. 
So, if $g=N$ interactions are global; as $g$ gets smaller interactions are more local. 
In an interaction each individual must decide whether to escalate a fight or not. 
Following the classic hawk-dove game [@maynard-smith_Evolution_1982], if the focal individual fights and its partner does not, the focal gets as payoff the resource of value $V$ while its partner gets nothing; if both individuals restrain  from fighting they split up the resource in half; if both decide to fight they splitup the cost of fighting and the winner takes over the resource. 
We further assume that the probability of wining a fight for the focal individual depends on the difference in quality between her and her interacting partner; specifically it is given by $p_{ij}=1/(1+e^{-k_3(Q_i-Q_j)})$, where $k_3$ is a parameter defining
how strong the quality difference determines the winning probability; 
and $i$ and $j$ denote the the position in the population 
vector of the focal and its interacting partner respectively.

The decision of whether to escalate a fight against an interacting partner can be dependent 
on the size of the partner´s badge, the actual dependency is determined by 
the focal´s experiences through a learning process. We implement learning using 
the actor-critic aproach from reinforcement learning theory [RL; @sutton_Reinforcement_2018;@quinones_Reinforcement_2019; @leimar_Learning_2019].
Individuals estimate the reward (payoff) expected from  interacting with partners 
of different badge sizes (the critic in RL terminology). 
After every interaction they update the estimate of reward proportionally to the difference between their current estimate and the observed reward (prediction error $\delta$). Furthermore, they express different probabilities of escalatining (retreating) depending on
the badge size of their opponent (the actor in RL). They update the probability of 
retreating (escalating)  up or down depending on whether retreating (escalating) leads to an increase 
in the reward estimation. So, if a focal individual decides to escalate a fight against 
an individual with small badge, and that leads to an increase in the reward estimation,
the focal individual will increase the probability of escalating fights with 
individuals of small badges in the future. Given that badge size is a real number between 0 and 1,
the reward estimation, as well as the probability of retreating (escalating), must be
generalized across different values. To implement generalization we use the linear function
approximation method based on radial basis functions [@sutton_Reinforcement_2018]. Specifically, we pick $f$ feature centers, which are evenly spaced values along the badge size interval ([0,1]). Each one of these features centers is associated with a weight for reward estimation and 
tendency to play retreat in a given interaction. The reward estimation and the tendency 
to retreat are calculated (in every interaction) as the sum of the weights associated 
with each feature, weighted by the response triggered by the feature. 
The response of each feature center dimishes as a gaussian function  with the distance between the feature center and the badge size of the partner. Formally the reward estimation 
$\hat{R}$ when the focal individual ($i$) faces individual $j$ is given by,

\begin{equation}
\hat{R}_{ij} =  \sum_{z=1}^{c} x_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where $x_z$ is the weight of feature center $z$ on the reward estimation; and $\theta$ is
the width of the generalization function. Similarly, the tendency to retreat is given by sum 
of feature weigths, and the probability of associated with the tendency is obtained
by applying a logistic transformation. So formally, the log-odds to retreat
when facing individual $j$ is 

\begin{equation}
logit(q_{ij}) =  \sum_{z=1}^{c} y_z e^{(-\frac{|B_i-c_z|}{2\theta^2})} 
\end{equation}

where $y_z$ is the weight of feature center $z$ on the tendency to retreat.

After all their interactions, individuals in the population reproduce proportionally to 
their fitness $w_i$, which is a sum of the baseline fitness ($w_0$) and all the payoffs
obtained throughout their life. Thus, the combination of natural selection and genetic drift
changes the distribution of values in $\alpha$ and $\beta$ that segregates in the population, effectively the badges expressed and changing the communication system. 

```{r Figure 1,echo=FALSE,fig.width=8,fig.align='center',fig.height=4,fig.show='hold',fig.cap= "Model of communication in the context of aggressive interactions. In A the sender code: a reaction norm determines how the badge size is determined by the quality of the individual. Red shows an honest reaction norm, while the blue shows a dishonest reaction norm. B the receiver code: individuals have a behavioral reaction norm that determines their probability of retreating (black line). The reaction norm arises from generalizing the information from the feature weights (black dots). Generalization is represented by the grey line and its axis, which shows the response triggered by the fourth feature diminishes as the value evaluated is further from the feature center. The learning process moves the feature weights (black dots) up or down depending on whether that leads to an increase in the estimated reward."}
honestBig<-readPNG(here("Images/cart_honestBig.png"))
honestSmall<-readPNG(here("Images/cart_honestSmall.png"))
dishonest<-readPNG(here("Images/cart_dishonest.png"))
par(plt=posPlot(numplotx = 2,idplotx = 1,upboundx = 91),las=1)
  rangPars<-data.frame(honest=c(4,2),dishonest=c(0,-1.8))
 rangQual<-seq(0,1,by = 0.01)
 linesSender<-sapply(rangPars,function(x)logist(rangQual,alpha = x[2],beta = x[1]))
 plot(0,0,type="l",xlab="",xlim=c(0,1),
      ylab ="",cex.lab=1,lwd=2,col="grey",cex.axis=1,ylim=c(0,1.1))
 text(x = 0.08,y=1.05,"A",cex = 1.5)
 mtext("Quality",1,line = 2,cex = 1,las = 0)
 matlines(x=rangQual,y=linesSender,lwd=4,col=colRuns,lty=1)
 mtext("Badge size",2,line = 2.3,cex = 1,las = 0)
 rasterImage(honestBig,0.75,0.6,0.95,0.8)
 rasterImage(honestSmall,0.05,0.01,0.25,0.21)
 rasterImage(dishonest,0.75,0.87,0.95,1.07)
 rasterImage(dishonest,0.05,0.87,0.25,1.07)
 # legend("left",legend=rangeBeta,col=colRuns,lwd=3,title = expression(beta),cex=2,bty="n")
 
 par(plt=posPlot(numplotx = 2,idplotx = 2,upboundx = 91),las=1,new=TRUE)
 
 nCenters<-6
interv<-1/(nCenters-1)
centers<-interv*seq(0,nCenters-1)
sigSq<-0.01
weights<-rep(0,nCenters)
weights<-c(-5,-2,-1,1,2,5)#runif(nCenters,min=-1,max=1)
rangx<-seq(0,1,length=1000)

plot(logist(totRBF(rangx,centers,sigSq,weights),alpha = 0,beta=1)~rangx,type='l',col=1,
     xlab="",ylab="",ylim=c(0,1.1),lwd=3,xaxt="s",cex.lab=2,cex.axis=1,yaxt='n')
axis(4,cex=1)
text(x = 0.08,y=1.05,"B",cex = 1.5)
# test<-sapply(1:6, function(x){RBF()})
points(y=logist(weights,alpha = 0,beta=1),x=centers,cex=1.5,pch=20)
respon<-logist(weights[4]*RBF(rangx,centers[4],0.01))
lines(x=rangx,y=respon,col="grey",lwd=2)
axis(side = 4,at = seq(range(respon)[1],range(respon)[2],length.out = 4),
     labels = round(seq(0,1,length.out = 4),2),line = -4,col="grey")

# xRBFs<-sapply(centers, function(x){seq(x-0.2,x+0.2,by=0.001)})
# RBFS<-sapply(1:6, function(x){
#   logist(weights[x]*RBF(seq(centers[x]-0.2,centers[x]+0.2,by=0.001),xCenter = centers[x],
#                         sigSq = 0.01))
# })
# matlines(x=xRBFs,y=RBFS,col="grey",lty=2,lwd=2)

mtext("prob. retreat",4,line = 2.3,cex = 1,las = 0)
mtext("Badge size",1,line = 2,cex = 1,las = 0)

 
 

```


## Results






## References